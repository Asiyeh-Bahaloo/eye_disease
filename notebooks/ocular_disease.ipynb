{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Copy of ocular-disease.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LlUL2kNe-kY7",
        "QoiDxZb7Aea6",
        "jCucceFC_lrq",
        "UkCJUWWU_B37",
        "owM0x98A_aWR",
        "rY0N_r-K_rqI",
        "iGyd40DKAhxg",
        "6HtOJEk8Bv6E",
        "IyXbFL8NBp4V",
        "i1pt16IACCM1",
        "oFnhWxFaAthx",
        "R45e13hd-pqE",
        "aCesUE3S2Mw1",
        "x358Ascp0qDs",
        "ClY3s8X60uR1",
        "UPRUvGvv0x4t",
        "ho_CfYlpUZ7o",
        "kfYV93pSVtPx",
        "vLqXL726UlgX",
        "uGXgYxBUmHvc",
        "kBmN0B0Vrcdn",
        "r9OAGC5amooW",
        "DulHadWqncAH",
        "u5Vvf701mqir",
        "hB9tbbwunfBI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Source\n",
        "\n",
        "[GitHub](https://github.com/JordiCorbilla/ocular-disease-intelligent-recognition-deep-learning)\n",
        "\n"
      ],
      "metadata": {
        "id": "-Ez3WETd3RJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Processing"
      ],
      "metadata": {
        "id": "LlUL2kNe-kY7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!ls ODIR-5K_Testing_Images_treated_224/| wc -l"
      ],
      "outputs": [],
      "metadata": {
        "id": "BoN5miL3rY2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Image Treatment Process"
      ],
      "metadata": {
        "id": "QoiDxZb7Aea6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image cropper"
      ],
      "metadata": {
        "id": "jCucceFC_lrq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# odir_image_crop.py \r\n",
        "import cv2\r\n",
        "import numpy as np\r\n",
        "import logging\r\n",
        "import os\r\n",
        "\r\n",
        "\r\n",
        "class ImageCrop:\r\n",
        "    def __init__(self, source_folder, destination_folder, file_name):\r\n",
        "        self.logger = logging.getLogger('odir')\r\n",
        "        self.source_folder = source_folder\r\n",
        "        self.destination_folder = destination_folder\r\n",
        "        self.file_name = file_name\r\n",
        "\r\n",
        "    def remove_black_pixels(self):\r\n",
        "        file = os.path.join(self.source_folder, self.file_name)\r\n",
        "        image = cv2.imread(file)\r\n",
        "\r\n",
        "        # Mask of coloured pixels.\r\n",
        "        mask = image > 0\r\n",
        "\r\n",
        "        # Coordinates of coloured pixels.\r\n",
        "        coordinates = np.argwhere(mask)\r\n",
        "\r\n",
        "        # Binding box of non-black pixels.\r\n",
        "        x0, y0, s0 = coordinates.min(axis=0)\r\n",
        "        x1, y1, s1 = coordinates.max(axis=0) + 1  # slices are exclusive at the top\r\n",
        "\r\n",
        "        # Get the contents of the bounding box.\r\n",
        "        cropped = image[x0:x1, y0:y1]\r\n",
        "        # overwrite the same file\r\n",
        "        file_cropped = os.path.join(self.destination_folder, self.file_name)\r\n",
        "        cv2.imwrite(file_cropped, cropped)"
      ],
      "outputs": [],
      "metadata": {
        "id": "aQ-jEoh258LM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing black pixels"
      ],
      "metadata": {
        "id": "UkCJUWWU_B37"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import logging\n",
        "import logging.config\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "# Note that this will alter the current training image set folder\n",
        "\n",
        "def process_all_images():\n",
        "    files = [f for f in listdir(source_folder) if isfile(join(source_folder, f))]\n",
        "    for file in files:\n",
        "        logger.debug('Processing image: ' + file)\n",
        "        ImageCrop(source_folder, destination_folder, file).remove_black_pixels()\n",
        "\n",
        "source_folder = r'ODIR-5K_Training_Dataset'\n",
        "destination_folder = r'ODIR-5K_Training_Dataset_cropped'\n",
        "# create logger\n",
        "logging.config.fileConfig('logging.conf')\n",
        "logger = logging.getLogger('odir')\n",
        "process_all_images()"
      ],
      "outputs": [],
      "metadata": {
        "id": "wJFhr7ie-mAu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import logging\n",
        "import logging.config\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "# Note that this will alter the current training image set folder\n",
        "\n",
        "def process_all_images():\n",
        "    files = [f for f in listdir(source_folder) if isfile(join(source_folder, f))]\n",
        "    for file in files:\n",
        "        logger.debug('Processing image: ' + file)\n",
        "        ImageCrop(source_folder, destination_folder, file).remove_black_pixels()\n",
        "\n",
        "\n",
        "source_folder = r'ODIR-5K_Testing_Images'\n",
        "destination_folder = r'ODIR-5K_Testing_Images_cropped'\n",
        "# create logger\n",
        "logging.config.fileConfig('logging.conf')\n",
        "logger = logging.getLogger('odir')\n",
        "process_all_images()"
      ],
      "outputs": [],
      "metadata": {
        "id": "589-U3Pg-msE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image resizer"
      ],
      "metadata": {
        "id": "owM0x98A_aWR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# odir_image_resizer.py\n",
        "import logging\n",
        "import PIL\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# This class allows you to resize and mirror an image of the dataset according to specific rules\n",
        "\n",
        "\n",
        "class ImageResizer:\n",
        "    def __init__(self, image_width, quality, source_folder, destination_folder, file_name, keep_aspect_ratio):\n",
        "        self.logger = logging.getLogger('odir')\n",
        "        self.image_width = image_width\n",
        "        self.quality = quality\n",
        "        self.source_folder = source_folder\n",
        "        self.destination_folder= destination_folder\n",
        "        self.file_name = file_name\n",
        "        self.keep_aspect_ration = keep_aspect_ratio\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\" Runs the image library using the constructor arguments.\n",
        "        Args:\n",
        "          No arguments are required.\n",
        "        Returns:\n",
        "          Saves the treated image into a separate folder.\n",
        "        \"\"\"\n",
        "        # We load the original file, we resize it to a smaller width and correspondent height and\n",
        "        # also mirror the image when we find a right eye image so they are all left eyes\n",
        "\n",
        "        file = os.path.join(self.source_folder, self.file_name)\n",
        "        img = Image.open(file)\n",
        "        if self.keep_aspect_ration:\n",
        "            # it will have the exact same width-to-height ratio as the original photo\n",
        "            width_percentage = (self.image_width / float(img.size[0]))\n",
        "            height_size = int((float(img.size[1]) * float(width_percentage)))\n",
        "            img = img.resize((self.image_width, height_size), PIL.Image.ANTIALIAS)\n",
        "        else:\n",
        "            # This will force the image to be square\n",
        "            img = img.resize((self.image_width, self.image_width), PIL.Image.ANTIALIAS)\n",
        "        if \"right\" in self.file_name:\n",
        "            self.logger.debug(\"Right eye image found. Flipping it\")\n",
        "            img.transpose(Image.FLIP_LEFT_RIGHT).save(os.path.join(self.destination_folder, self.file_name), optimize=True, quality=self.quality)\n",
        "        else:\n",
        "            img.save(os.path.join(self.destination_folder, self.file_name), optimize=True, quality=self.quality)\n",
        "        self.logger.debug(\"Image saved\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "j-ugNSWT_ZHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resizing the images to 224 pixels"
      ],
      "metadata": {
        "id": "rY0N_r-K_rqI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import logging\n",
        "import logging.config\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "# This default job to 224px images, will shrink the dataset from 1,439,776,768 bytes\n",
        "# to 116,813,824 bytes 91.8% size reduction\n",
        "\n",
        "\n",
        "def process_all_images():\n",
        "    files = [f for f in listdir(source_folder) if isfile(join(source_folder, f))]\n",
        "    for file in files:\n",
        "        logger.debug('Processing image: ' + file)\n",
        "        ImageResizer(image_width, quality, source_folder, destination_folder, file, keep_aspect_ratio).run()\n",
        "\n",
        "\n",
        "# Set the base width of the image to 200 pixels\n",
        "image_width = 224\n",
        "keep_aspect_ratio = False\n",
        "# set the quality of the resultant jpeg to 100%\n",
        "quality = 100\n",
        "source_folder = r'ODIR-5K_Training_Dataset_cropped'\n",
        "destination_folder = r'ODIR-5K_Training_Dataset_treated' + '_' + str(image_width)\n",
        "# create logger\n",
        "logging.config.fileConfig('logging.conf')\n",
        "logger = logging.getLogger('odir')\n",
        "process_all_images()"
      ],
      "outputs": [],
      "metadata": {
        "id": "4CcrMV2M--vP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import logging\n",
        "import logging.config\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "# This default job to 224px images, will shrink the dataset from 1,439,776,768 bytes\n",
        "# to 116,813,824 bytes 91.8% size reduction\n",
        "\n",
        "def process_all_images():\n",
        "    files = [f for f in listdir(source_folder) if isfile(join(source_folder, f))]\n",
        "    for file in files:\n",
        "        logger.debug('Processing image: ' + file)\n",
        "        ImageResizer(image_width, quality, source_folder, destination_folder, file, keep_aspect_ratio).run()\n",
        "\n",
        "\n",
        "# Set the base width of the image to 200 pixels\n",
        "image_width = 224\n",
        "keep_aspect_ratio = False\n",
        "# set the quality of the resultant jpeg to 100%\n",
        "quality = 100\n",
        "source_folder = r'ODIR-5K_Testing_Images_cropped'\n",
        "destination_folder = r'ODIR-5K_Testing_Images_treated' + '_' + str(image_width)\n",
        "# create logger\n",
        "logging.config.fileConfig('logging.conf')\n",
        "logger = logging.getLogger('odir')\n",
        "process_all_images()"
      ],
      "outputs": [],
      "metadata": {
        "id": "SnjFO7Fo_ywL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Augmentation"
      ],
      "metadata": {
        "id": "iGyd40DKAhxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image treatment"
      ],
      "metadata": {
        "id": "6HtOJEk8Bv6E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from skimage import exposure\n",
        "\n",
        "\n",
        "class ImageTreatment:\n",
        "    def __init__(self, image_size):\n",
        "        self.image_size = image_size\n",
        "\n",
        "    def scaling(self, image, scale_vector):\n",
        "        # Resize to 4-D vector\n",
        "        image = np.reshape(image, (1, self.image_size, self.image_size, 3))\n",
        "        boxes = np.zeros((len(scale_vector), 4), dtype=np.float32)\n",
        "        for index, scale in enumerate(scale_vector):\n",
        "            x1 = y1 = 0.5 - 0.5 * scale\n",
        "            x2 = y2 = 0.5 + 0.5 * scale\n",
        "            boxes[index] = np.array([y1, x1, y2, x2], dtype=np.float32)\n",
        "        box_ind = np.zeros((len(scale_vector)), dtype=np.int32)\n",
        "        crop_size = np.array([self.image_size, self.image_size], dtype=np.int32)\n",
        "\n",
        "        output = tf.image.crop_and_resize(image, boxes, box_ind, crop_size)\n",
        "        output = np.array(output, dtype=np.uint8)\n",
        "        return output\n",
        "\n",
        "    def brightness(self, image, delta):\n",
        "        output = tf.image.adjust_brightness(image, delta)\n",
        "        output = np.array(output, dtype=np.uint8)\n",
        "        return output\n",
        "\n",
        "    def contrast(self, image, contrast_factor):\n",
        "        output = tf.image.adjust_contrast(image, contrast_factor)\n",
        "        output = np.array(output, dtype=np.uint8)\n",
        "        return output\n",
        "\n",
        "    def saturation(self, image, saturation_factor):\n",
        "        output = tf.image.adjust_saturation(image, saturation_factor)\n",
        "        output = np.array(output, dtype=np.uint8)\n",
        "        return output\n",
        "\n",
        "    def hue(self, image, delta):\n",
        "        output = tf.image.adjust_hue(image, delta)\n",
        "        output = np.array(output, dtype=np.uint8)\n",
        "        return output\n",
        "\n",
        "    def central_crop(self, image, central_fraction):\n",
        "        output = tf.image.central_crop(image, central_fraction)\n",
        "        output = np.array(output, dtype=np.uint8)\n",
        "        return output\n",
        "\n",
        "    def crop_to_bounding_box(self, image, offset_height, offset_width, target_height, target_width):\n",
        "        output = tf.image.crop_to_bounding_box(image, offset_height, offset_width, target_height, target_width)\n",
        "        output = tf.image.resize(output, (self.image_size, self.image_size))\n",
        "        output = np.array(output, dtype=np.uint8)\n",
        "        return output\n",
        "\n",
        "    def gamma(self, image, gamma):\n",
        "        output = tf.image.adjust_gamma(image, gamma)\n",
        "        output = np.array(output, dtype=np.uint8)\n",
        "        return output\n",
        "\n",
        "    def rot90(self, image, k):\n",
        "        output = tf.image.rot90(image, k)\n",
        "        output = np.array(output, dtype=np.uint8)\n",
        "        return output\n",
        "\n",
        "    def rescale_intensity(self, image):\n",
        "        p2, p98 = np.percentile(image, (2, 98))\n",
        "        img_rescale = exposure.rescale_intensity(image, in_range=(p2, p98))\n",
        "        return img_rescale\n",
        "\n",
        "    def equalize_histogram(self, image):\n",
        "        img_eq = exposure.equalize_hist(image)\n",
        "        return img_eq\n",
        "\n",
        "    def equalize_adapthist(self, image):\n",
        "        img_adapted = exposure.equalize_adapthist(image, clip_limit=0.03)\n",
        "        return img_adapted"
      ],
      "outputs": [],
      "metadata": {
        "id": "F1m9ZrysBmd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data augmentation strategy"
      ],
      "metadata": {
        "id": "IyXbFL8NBp4V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import csv\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "class DataAugmentationStrategy:\n",
        "    def __init__(self, image_size, file_name):\n",
        "        self.base_image = file_name\n",
        "        self.treatment = ImageTreatment(image_size)\n",
        "        self.file_path = r'ODIR-5K_Training_Dataset_treated_' + str(image_size)\n",
        "        self.saving_path = r'ODIR-5K_Training_Dataset_augmented_' + str(image_size)\n",
        "        self.file_id = file_name.replace('.jpg', '')\n",
        "\n",
        "    def save_image(self, original_vector, image, sample):\n",
        "        central = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        file = self.file_id + '_'+str(sample)+'.jpg'\n",
        "        file_name = os.path.join(self.saving_path, file)\n",
        "        exists = os.path.isfile(file_name)\n",
        "        if exists:\n",
        "            print(\"duplicate file found: \" + file_name)\n",
        "\n",
        "        status = cv2.imwrite(file_name, central)\n",
        "\n",
        "        with open(r'ground_truth\\odir_augmented.csv', 'a', newline='') as csv_file:\n",
        "            file_writer = csv.writer(csv_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "            file_writer.writerow([file, original_vector[1], original_vector[2], original_vector[3], original_vector[4],\n",
        "                                   original_vector[5], original_vector[6], original_vector[7], original_vector[8]])\n",
        "\n",
        "        #print(file_name + \" written to file-system : \", status)\n",
        "\n",
        "    def generate_images(self, number_samples, original_vector, weights):\n",
        "        eye_image = os.path.join(self.file_path, self.base_image)\n",
        "        image = cv2.imread(eye_image)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        original_image = image\n",
        "        saved = 0\n",
        "\n",
        "        # For any repeating elements, just give the other output\n",
        "        # We are only expecting up to 3 repetitions\n",
        "        if weights == 20:\n",
        "            original_image = self.treatment.rot90(original_image, 2)\n",
        "        if weights == 400:\n",
        "            original_image = self.treatment.rot90(original_image, 3)\n",
        "        if weights > 401:\n",
        "            print(str(self.file_id) + ' samples:' + str(number_samples))\n",
        "            raise ValueError('this cannot happen')\n",
        "\n",
        "        # for the sample type 14, just generate 1 image and leave the method\n",
        "        if number_samples == 14:\n",
        "            central = self.treatment.rot90(original_image, 1)\n",
        "            self.save_image(original_vector, central, weights+14)\n",
        "            saved = saved +1\n",
        "            return saved\n",
        "\n",
        "        if number_samples > 0:\n",
        "            central = self.treatment.crop_to_bounding_box(original_image, 0, 0, 112, 112)\n",
        "            self.save_image(original_vector, central, weights+0)\n",
        "            saved = saved + 1\n",
        "\n",
        "        if number_samples > 1:\n",
        "            central = self.treatment.crop_to_bounding_box(original_image, 112, 0, 112, 112)\n",
        "            self.save_image(original_vector, central, weights+1)\n",
        "            saved = saved + 1\n",
        "\n",
        "        if number_samples > 2:\n",
        "            central = self.treatment.crop_to_bounding_box(original_image, 0, 112, 112, 112)\n",
        "            self.save_image(original_vector, central, weights+2)\n",
        "            saved = saved + 1\n",
        "\n",
        "        if number_samples > 3:\n",
        "            central = self.treatment.crop_to_bounding_box(original_image, 112, 112, 112, 112)\n",
        "            self.save_image(original_vector, central, weights+3)\n",
        "            saved = saved + 1\n",
        "\n",
        "        if number_samples > 4:\n",
        "            vector = [0.50]\n",
        "            central = self.treatment.scaling(original_image, vector)\n",
        "            self.save_image(original_vector, central[0], weights+4)\n",
        "            saved = saved + 1\n",
        "\n",
        "        if number_samples > 5:\n",
        "            vector = [0.70]\n",
        "            central = self.treatment.scaling(original_image, vector)\n",
        "            self.save_image(original_vector, central[0], weights+5)\n",
        "            saved = saved + 1\n",
        "\n",
        "        if number_samples > 6:\n",
        "            vector = [0.80]\n",
        "            central = self.treatment.scaling(original_image, vector)\n",
        "            self.save_image(original_vector, central[0], weights+6)\n",
        "            saved = saved + 1\n",
        "\n",
        "        if number_samples > 7:\n",
        "            vector = [0.90]\n",
        "            central = self.treatment.scaling(original_image, vector)\n",
        "            self.save_image(original_vector, central[0], weights+7)\n",
        "            saved = saved + 1\n",
        "\n",
        "        if number_samples > 8:\n",
        "            central = self.treatment.rescale_intensity(original_image)\n",
        "            self.save_image(original_vector, central, weights+8)\n",
        "            saved = saved + 1\n",
        "\n",
        "        if number_samples > 9:\n",
        "            central = self.treatment.contrast(original_image, 2)\n",
        "            self.save_image(original_vector, central, weights+9)\n",
        "            saved = saved + 1\n",
        "\n",
        "        if number_samples > 10:\n",
        "            central = self.treatment.saturation(original_image, 0.5)\n",
        "            self.save_image(original_vector, central, weights+10)\n",
        "            saved = saved + 1\n",
        "\n",
        "        if number_samples > 11:\n",
        "            central = self.treatment.gamma(original_image, 0.5)\n",
        "            self.save_image(original_vector, central, weights+11)\n",
        "            saved = saved + 1\n",
        "\n",
        "        if number_samples > 12:\n",
        "            central = self.treatment.hue(original_image, 0.2)\n",
        "            self.save_image(original_vector, central, weights+12)\n",
        "            saved = saved + 1\n",
        "\n",
        "        return saved"
      ],
      "outputs": [],
      "metadata": {
        "id": "lCMF2exZBQud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Groung truth files"
      ],
      "metadata": {
        "id": "i1pt16IACCM1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import csv\n",
        "\n",
        "class GroundTruthFiles:\n",
        "    def __init__(self):\n",
        "        self.amd = []\n",
        "        self.cataract = []\n",
        "        self.diabetes = []\n",
        "        self.glaucoma = []\n",
        "        self.hypertension = []\n",
        "        self.myopia = []\n",
        "        self.others = []\n",
        "\n",
        "    def populate_vectors(self, ground_truth_file):\n",
        "        with open(ground_truth_file) as csvDataFile:\n",
        "            csv_reader = csv.reader(csvDataFile)\n",
        "\n",
        "            for row in csv_reader:\n",
        "                column_id = row[0]\n",
        "                normal = row[1]\n",
        "                diabetes = row[2]\n",
        "                glaucoma = row[3]\n",
        "                cataract = row[4]\n",
        "                amd = row[5]\n",
        "                hypertension = row[6]\n",
        "                myopia = row[7]\n",
        "                others = row[8]\n",
        "                # just discard the first row\n",
        "                if column_id != \"ID\":\n",
        "                    print(\"Processing image: \" + column_id + \"_left.jpg\")\n",
        "                    if diabetes == '1':\n",
        "                        self.diabetes.append([column_id, normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
        "                    if glaucoma == '1':\n",
        "                        self.glaucoma.append([column_id, normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
        "                    if cataract == '1':\n",
        "                        self.cataract.append([column_id, normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
        "                    if amd == '1':\n",
        "                        self.amd.append([column_id, normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
        "                    if hypertension == '1':\n",
        "                        self.hypertension.append([column_id, normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
        "                    if myopia == '1':\n",
        "                        self.myopia.append([column_id, normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
        "                    if others == '1':\n",
        "                        self.others.append([column_id, normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])"
      ],
      "outputs": [],
      "metadata": {
        "id": "k941ncM1CDkL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import csv\n",
        "import logging.config\n",
        "import os\n",
        "from absl import app\n",
        "\n",
        "\n",
        "def write_header():\n",
        "    with open(r'ground_truth/odir_augmented.csv', 'w', newline='') as csv_file:\n",
        "        file_writer = csv.writer(csv_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "        file_writer.writerow(['ID', 'Normal', 'Diabetes', 'Glaucoma', 'Cataract', 'AMD', 'Hypertension',\n",
        "                              'Myopia', 'Others'])\n",
        "        return file_writer\n",
        "\n",
        "\n",
        "def process_files(images, cache, files):\n",
        "    total = 0\n",
        "    for strategy in range(len(images)):\n",
        "        images_to_process = images[strategy][0]\n",
        "        samples_per_image = images[strategy][1]\n",
        "        for image_index in range(images_to_process):\n",
        "            image_vector = files[image_index]\n",
        "            file_name = image_vector[0]\n",
        "\n",
        "            # Only check during the first strategy\n",
        "            if strategy == 0:\n",
        "                if file_name not in cache:\n",
        "                    cache[file_name] = 1\n",
        "                else:\n",
        "                    cache[file_name] = cache[file_name] * 20\n",
        "\n",
        "            # print('Processing: ' + file_name)\n",
        "            augment = DataAugmentationStrategy(image_size, file_name)\n",
        "            count = augment.generate_images(samples_per_image, image_vector, cache[file_name])\n",
        "            total = total + count\n",
        "    return total\n",
        "\n",
        "\n",
        "# def main(argv):\n",
        "def main():\n",
        "    # load the ground truth file\n",
        "    files = GroundTruthFiles()\n",
        "    files.populate_vectors(csv_path)\n",
        "\n",
        "    print('files record count order by size ASC')\n",
        "    print('hypertension ' + str(len(files.hypertension)))\n",
        "    print('myopia ' + str(len(files.myopia)))\n",
        "    print('cataract ' + str(len(files.cataract)))\n",
        "    print('amd ' + str(len(files.amd)))\n",
        "    print('glaucoma ' + str(len(files.glaucoma)))\n",
        "    print('others ' + str(len(files.others)))\n",
        "    print('diabetes ' + str(len(files.diabetes)))\n",
        "\n",
        "    images_hypertension = [[len(files.hypertension), 13], [128, 14]]\n",
        "    images_myopia = [[len(files.myopia), 9], [196, 14]]\n",
        "    images_cataract = [[len(files.cataract), 9], [66, 14]]\n",
        "    images_amd = [[len(files.amd), 9], [16, 14]]\n",
        "    images_glaucoma = [[len(files.glaucoma), 7], [312, 14]]\n",
        "    images_others = [[len(files.others), 1], [568, 14]]\n",
        "    images_diabetes = [[1038, 1]]\n",
        "\n",
        "    # Delete previous file\n",
        "    exists = os.path.isfile(r'ground_truth/odir_augmented.csv')\n",
        "    if exists:\n",
        "        os.remove(r'ground_truth/odir_augmented.csv')\n",
        "\n",
        "    write_header()\n",
        "\n",
        "    images_processed = {}\n",
        "\n",
        "    total_hypertension = process_files(images_hypertension, images_processed, files.hypertension)\n",
        "    total_myopia = process_files(images_myopia, images_processed, files.myopia)\n",
        "    total_cataract = process_files(images_cataract, images_processed, files.cataract)\n",
        "    total_amd = process_files(images_amd, images_processed, files.amd)\n",
        "    total_glaucoma = process_files(images_glaucoma, images_processed, files.glaucoma)\n",
        "    total_others = process_files(images_others, images_processed, files.others)\n",
        "    total_diabetes = process_files(images_diabetes, images_processed, files.diabetes)\n",
        "\n",
        "    print(\"total generated hypertension: \" + str(total_hypertension))\n",
        "    print(\"total generated myopia: \" + str(total_myopia))\n",
        "    print(\"total generated cataract: \" + str(total_cataract))\n",
        "    print(\"total generated amd: \" + str(total_amd))\n",
        "    print(\"total generated glaucoma: \" + str(total_glaucoma))\n",
        "    print(\"total generated others: \" + str(total_others))\n",
        "    print(\"total generated diabetes: \" + str(total_diabetes))\n",
        "\n",
        "# create logger\n",
        "logging.config.fileConfig('logging.conf')\n",
        "logger = logging.getLogger('odir')\n",
        "image_size = 224\n",
        "csv_path = 'ground_truth/odir.csv'"
      ],
      "outputs": [],
      "metadata": {
        "id": "40F3WqAWBIsM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# app.run(main)\n",
        "main()"
      ],
      "outputs": [],
      "metadata": {
        "id": "jsGbbCKsqWkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Image to tf.Data conversion and .npy storage"
      ],
      "metadata": {
        "id": "oFnhWxFaAthx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from absl import app\n",
        "import logging\n",
        "import logging.config\n",
        "import time\n",
        "import csv\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "\n",
        "class NumpyDataGenerator:\n",
        "    def __init__(self, training_path, testing_path, csv_path, csv_testing_path, augmented_path, csv_augmented_file):\n",
        "        self.training_path = training_path\n",
        "        self.testing_path = testing_path\n",
        "        self.csv_path = csv_path\n",
        "        self.csv_testing_path = csv_testing_path\n",
        "        self.logger = logging.getLogger('odir')\n",
        "        self.total_records_training = 0\n",
        "        self.total_records_testing = 0\n",
        "        self.csv_augmented_path = csv_augmented_file\n",
        "        self.augmented_path = augmented_path\n",
        "\n",
        "    def npy_training_files(self, file_name_training, file_name_training_labels):\n",
        "        training = []\n",
        "        training_labels = []\n",
        "\n",
        "        self.logger.debug(\"Opening CSV file\")\n",
        "        with open(self.csv_path) as csvDataFile:\n",
        "            csv_reader = csv.reader(csvDataFile)\n",
        "            self.total_records_training = 0\n",
        "            for row in csv_reader:\n",
        "                column_id = row[0]\n",
        "                normal = row[1]\n",
        "                diabetes = row[2]\n",
        "                glaucoma = row[3]\n",
        "                cataract = row[4]\n",
        "                amd = row[5]\n",
        "                hypertension = row[6]\n",
        "                myopia = row[7]\n",
        "                others = row[8]\n",
        "                # just discard the first row\n",
        "                if column_id != \"ID\":\n",
        "                    self.logger.debug(\"Processing image: \" + column_id)\n",
        "                    # load first the image from the folder\n",
        "                    eye_image = os.path.join(self.training_path, column_id)\n",
        "                    image = cv2.imread(eye_image)\n",
        "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                    training.append(image)\n",
        "                    training_labels.append([normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
        "                    self.total_records_training = self.total_records_training + 1\n",
        "\n",
        "        training = np.array(training, dtype='uint8')\n",
        "        training_labels = np.array(training_labels, dtype='uint8')\n",
        "        # convert (number of images x height x width x number of channels) to (number of images x (height * width *3))\n",
        "        # for example (6069 * 28 * 28 * 3)-> (6069 x 2352) (14,274,288)\n",
        "        training = np.reshape(training, [training.shape[0], training.shape[1], training.shape[2], training.shape[3]])\n",
        "\n",
        "        # save numpy array as .npy formats\n",
        "        np.save(file_name_training, training)\n",
        "        self.logger.debug(\"Saving NPY File: \" + file_name_training)\n",
        "        np.save(file_name_training_labels, training_labels)\n",
        "        self.logger.debug(\"Saving NPY File: \" + file_name_training_labels)\n",
        "        self.logger.debug(\"Closing CSV file\")\n",
        "\n",
        "    def npy_testing_files(self, file_name_testing, file_name_testing_labels):\n",
        "        testing = []\n",
        "        testing_labels = []\n",
        "\n",
        "        self.logger.debug(\"Opening CSV file\")\n",
        "        with open(self.csv_testing_path) as csvDataFile:\n",
        "            csv_reader = csv.reader(csvDataFile)\n",
        "            self.total_records_testing = 0\n",
        "            for row in csv_reader:\n",
        "                column_id = row[0]\n",
        "                normal = row[1]\n",
        "                diabetes = row[2]\n",
        "                glaucoma = row[3]\n",
        "                cataract = row[4]\n",
        "                amd = row[5]\n",
        "                hypertension = row[6]\n",
        "                myopia = row[7]\n",
        "                others = row[8]\n",
        "                # just discard the first row\n",
        "                if column_id != \"ID\":\n",
        "                    self.logger.debug(\"Processing image: \" + column_id + \"_left.jpg\")\n",
        "                    # load first the image from the folder\n",
        "                    eye_image = os.path.join(self.testing_path, column_id + \"_left.jpg\")\n",
        "                    image = cv2.imread(eye_image)\n",
        "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                    testing.append(image)\n",
        "                    testing_labels.append([normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
        "                    self.total_records_testing = self.total_records_testing + 1\n",
        "\n",
        "                    self.logger.debug(\"Processing image: \" + column_id + \"_right.jpg\")\n",
        "                    eye_image = os.path.join(self.testing_path, column_id + \"_right.jpg\")\n",
        "                    image = cv2.imread(eye_image)\n",
        "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                    testing.append(image)\n",
        "                    testing_labels.append([normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
        "                    self.total_records_testing = self.total_records_testing + 1\n",
        "\n",
        "        testing = np.array(testing, dtype='uint8')\n",
        "        training_labels = np.array(testing_labels, dtype='uint8')\n",
        "        # convert (number of images x height x width x number of channels) to (number of images x (height * width *3))\n",
        "        # for example (6069 * 28 * 28 * 3)-> (6069 x 2352) (14,274,288)\n",
        "        testing = np.reshape(testing, [testing.shape[0], testing.shape[1], testing.shape[2], testing.shape[3]])\n",
        "\n",
        "        # save numpy array as .npy formats\n",
        "        np.save(file_name_testing, testing)\n",
        "        self.logger.debug(\"Saving NPY File: \" + file_name_testing)\n",
        "        np.save(file_name_testing_labels, training_labels)\n",
        "        self.logger.debug(\"Saving NPY File: \" + file_name_testing_labels)\n",
        "        self.logger.debug(\"Closing CSV file\")\n",
        "\n",
        "    def npy_training_files_split(self, split_number, file_name_training, file_name_training_labels, file_name_testing,\n",
        "                                 file_name_testing_labels):\n",
        "        training = []\n",
        "        training_labels = []\n",
        "        testing = []\n",
        "        testing_labels = []\n",
        "\n",
        "        self.logger.debug(\"Opening CSV file\")\n",
        "        count = 0\n",
        "        with open(self.csv_path) as csvDataFile:\n",
        "            csv_reader = csv.reader(csvDataFile)\n",
        "            self.total_records_training = 0\n",
        "            self.total_records_testing = 0\n",
        "            for row in csv_reader:\n",
        "                column_id = row[0]\n",
        "                label = row[1]\n",
        "                # just discard the first row\n",
        "                if column_id != \"ID\":\n",
        "                    self.logger.debug(\"Processing image: \" + column_id)\n",
        "                    # load first the image from the folder\n",
        "                    eye_image = os.path.join(self.training_path, column_id)\n",
        "                    image = cv2.imread(eye_image)\n",
        "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                    if count < split_number:\n",
        "                        testing.append(image)\n",
        "                        testing_labels.append(label)\n",
        "                        self.total_records_testing = self.total_records_testing + 1\n",
        "                    else:\n",
        "                        training.append(image)\n",
        "                        training_labels.append(label)\n",
        "                        self.total_records_training = self.total_records_training + 1\n",
        "                    count = count + 1\n",
        "\n",
        "        testing = np.array(testing, dtype='uint8')\n",
        "        testing_labels = np.array(testing_labels, dtype='uint8')\n",
        "        testing = np.reshape(testing, [testing.shape[0], testing.shape[1], testing.shape[2], testing.shape[3]])\n",
        "\n",
        "        # save numpy array as .npy formats\n",
        "        np.save(file_name_testing, testing)\n",
        "        np.save(file_name_testing_labels, testing_labels)\n",
        "\n",
        "        training = np.array(training, dtype='uint8')\n",
        "        training_labels = np.array(training_labels, dtype='uint8')\n",
        "        # convert (number of images x height x width x number of channels) to (number of images x (height * width *3))\n",
        "        # for example (6069 * 28 * 28 * 3)-> (6069 x 2352) (14,274,288)\n",
        "        training = np.reshape(training, [training.shape[0], training.shape[1], training.shape[2], training.shape[3]])\n",
        "\n",
        "        # save numpy array as .npy formats\n",
        "        np.save(file_name_training, training)\n",
        "        self.logger.debug(\"Saving NPY File: \" + file_name_training)\n",
        "        np.save(file_name_training_labels, training_labels)\n",
        "        self.logger.debug(\"Saving NPY File: \" + file_name_training_labels)\n",
        "        self.logger.debug(\"Closing CSV file\")\n",
        "\n",
        "    def is_sickness(self, row, sickness):\n",
        "        switcher = {\n",
        "            \"normal\": row[1] == '1' and row[2] == '0' and row[3] == '0' and row[4] == '0' and row[5] == '0' and row[\n",
        "                6] == '0' and row[7] == '0' and row[8] == '0',\n",
        "            \"diabetes\": row[1] == '0' and row[2] == '1' and row[3] == '0' and row[4] == '0' and row[5] == '0' and row[\n",
        "                6] == '0' and row[7] == '0' and row[8] == '0',\n",
        "            \"glaucoma\": row[1] == '0' and row[2] == '0' and row[3] == '1' and row[4] == '0' and row[5] == '0' and row[\n",
        "                6] == '0' and row[7] == '0' and row[8] == '0',\n",
        "            \"cataract\": row[1] == '0' and row[2] == '0' and row[3] == '0' and row[4] == '1' and row[5] == '0' and row[\n",
        "                6] == '0' and row[7] == '0' and row[8] == '0',\n",
        "            \"amd\": row[1] == '0' and row[2] == '0' and row[3] == '0' and row[4] == '0' and row[5] == '1' and row[\n",
        "                6] == '0' and row[7] == '0' and row[8] == '0',\n",
        "            \"hypertension\": row[1] == '0' and row[2] == '0' and row[3] == '0' and row[4] == '0' and row[5] == '0' and\n",
        "                            row[6] == '1' and row[7] == '0' and row[8] == '0',\n",
        "            \"myopia\": row[1] == '0' and row[2] == '0' and row[3] == '0' and row[4] == '0' and row[5] == '0' and row[\n",
        "                6] == '0' and row[7] == '1' and row[8] == '0',\n",
        "            \"others\": row[1] == '0' and row[2] == '0' and row[3] == '0' and row[4] == '0' and row[5] == '0' and row[\n",
        "                6] == '0' and row[7] == '0' and row[8] == '1'\n",
        "        }\n",
        "        return switcher.get(sickness, False)\n",
        "\n",
        "    def npy_training_files_split_all(self, split_number, file_name_training, file_name_training_labels,\n",
        "                                     file_name_testing,\n",
        "                                     file_name_testing_labels, include_augmented):\n",
        "        split_factor = 10820\n",
        "        training = []\n",
        "        training_labels = []\n",
        "        training_2 = []\n",
        "        training_labels_2 = []\n",
        "        testing = []\n",
        "        testing_labels = []\n",
        "        images_used = []\n",
        "        count_images = 0\n",
        "\n",
        "        class_names = ['normal', 'diabetes', 'glaucoma', 'cataract', 'amd',\n",
        "                       'hypertension', 'myopia', 'others']\n",
        "\n",
        "        self.logger.debug(\"Opening CSV file\")\n",
        "\n",
        "        class_count = {'normal': 0, 'diabetes': 0, 'glaucoma': 0, 'cataract': 0, 'amd': 0, 'hypertension': 0,\n",
        "                       'myopia': 0, 'others': 0}\n",
        "        split_pocket = split_number / 8\n",
        "        with open(self.csv_path) as csvDataFile:\n",
        "            csv_reader = csv.reader(csvDataFile)\n",
        "            self.total_records_training = 0\n",
        "            self.total_records_testing = 0\n",
        "            for row in csv_reader:\n",
        "                column_id = row[0]\n",
        "                normal = row[1]\n",
        "                diabetes = row[2]\n",
        "                glaucoma = row[3]\n",
        "                cataract = row[4]\n",
        "                amd = row[5]\n",
        "                hypertension = row[6]\n",
        "                myopia = row[7]\n",
        "                others = row[8]\n",
        "                # just discard the first row\n",
        "                if column_id != \"ID\":\n",
        "                    self.logger.debug(\"Processing image: \" + column_id)\n",
        "                    # load first the image from the folder\n",
        "                    eye_image = os.path.join(self.training_path, column_id)\n",
        "                    image = cv2.imread(eye_image)\n",
        "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                    found = False\n",
        "                    for sickness in class_names:\n",
        "                        if self.is_sickness(row, sickness) and class_count[sickness] < split_pocket:\n",
        "                            testing.append(image)\n",
        "                            images_used.append(row[0] + ',' + sickness + ',' + str(class_count[sickness]))\n",
        "                            testing_labels.append([normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
        "                            self.total_records_testing = self.total_records_testing + 1\n",
        "                            class_count[sickness] = class_count[sickness] + 1\n",
        "                            found = True\n",
        "                            logger.debug('found ' + sickness + ' ' + str(class_count[sickness]))\n",
        "\n",
        "                    if not found:\n",
        "                        training.append(image)\n",
        "                        training_labels.append([normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
        "                        self.total_records_training = self.total_records_training + 1\n",
        "                        count_images = count_images + 1\n",
        "\n",
        "        if include_augmented:\n",
        "            with open(self.csv_augmented_path) as csvDataFile:\n",
        "                csv_reader = csv.reader(csvDataFile)\n",
        "                for row in csv_reader:\n",
        "                    column_id = row[0]\n",
        "                    normal = row[1]\n",
        "                    diabetes = row[2]\n",
        "                    glaucoma = row[3]\n",
        "                    cataract = row[4]\n",
        "                    amd = row[5]\n",
        "                    hypertension = row[6]\n",
        "                    myopia = row[7]\n",
        "                    others = row[8]\n",
        "                    # just discard the first row\n",
        "                    if column_id != \"ID\":\n",
        "                        self.logger.debug(\"Processing image: \" + column_id)\n",
        "                        # load first the image from the folder\n",
        "                        eye_image = os.path.join(self.augmented_path, column_id)\n",
        "                        image = cv2.imread(eye_image)\n",
        "                        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                        if count_images >= split_factor:\n",
        "                            training_2.append(image)\n",
        "                            training_labels_2.append([normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
        "                        else:\n",
        "                            training.append(image)\n",
        "                            training_labels.append([normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
        "                        self.total_records_training = self.total_records_training + 1\n",
        "                        count_images = count_images + 1\n",
        "\n",
        "        testing = np.array(testing, dtype='uint8')\n",
        "        testing_labels = np.array(testing_labels, dtype='uint8')\n",
        "        testing = np.reshape(testing, [testing.shape[0], testing.shape[1], testing.shape[2], testing.shape[3]])\n",
        "\n",
        "        # save numpy array as .npy formats\n",
        "        np.save(file_name_testing, testing)\n",
        "        np.save(file_name_testing_labels, testing_labels)\n",
        "\n",
        "        training = np.array(training, dtype='uint8')\n",
        "        training_labels = np.array(training_labels, dtype='uint8')\n",
        "        # convert (number of images x height x width x number of channels) to (number of images x (height * width *3))\n",
        "        # for example (6069 * 28 * 28 * 3)-> (6069 x 2352) (14,274,288)\n",
        "        training = np.reshape(training, [training.shape[0], training.shape[1], training.shape[2], training.shape[3]])\n",
        "\n",
        "        # convert (number of images x height x width x number of channels) to (number of images x (height * width *3))\n",
        "        # for example (6069 * 28 * 28 * 3)-> (6069 x 2352) (14,274,288)\n",
        "        if include_augmented:\n",
        "            training_2 = np.array(training_2, dtype='uint8')\n",
        "            training_labels_2 = np.array(training_labels_2, dtype='uint8')\n",
        "            training_2 = np.reshape(training_2, [training_2.shape[0], training_2.shape[1], training_2.shape[2], training_2.shape[3]])\n",
        "\n",
        "        self.logger.debug(testing.shape)\n",
        "        self.logger.debug(testing_labels.shape)\n",
        "        self.logger.debug(training.shape)\n",
        "        self.logger.debug(training_labels.shape)\n",
        "        if include_augmented:\n",
        "            self.logger.debug(training_2.shape)\n",
        "            self.logger.debug(training_labels_2.shape)\n",
        "\n",
        "        # save numpy array as .npy formats\n",
        "        np.save(file_name_training + '_1', training)\n",
        "        np.save(file_name_training_labels + '_1', training_labels)\n",
        "        if include_augmented:\n",
        "            np.save(file_name_training + '_2', training_2)\n",
        "            np.save(file_name_training_labels + '_2', training_labels_2)\n",
        "        self.logger.debug(\"Closing CSV file\")\n",
        "        for sickness in class_names:\n",
        "            self.logger.debug('found ' + sickness + ' ' + str(class_count[sickness]))\n",
        "        csv_writer = csv.writer(open(\"files_used.csv\", 'w', newline=''))\n",
        "        for item in images_used:\n",
        "            self.logger.debug(item)\n",
        "            entries = item.split(\",\")\n",
        "            csv_writer.writerow(entries)\n",
        "\n",
        "\n",
        "# def main(argv):\n",
        "def main():\n",
        "    start = time.time()\n",
        "    image_width = 224\n",
        "    training_path = r'ODIR-5K_Training_Dataset_treated' + '_' + str(image_width)\n",
        "    testing_path = r'ODIR-5K_Testing_Images_treated' + '_' + str(image_width)\n",
        "    augmented_path = r'ODIR-5K_Training_Dataset_augmented' + '_' + str(image_width)\n",
        "    csv_file = r'ground_truth/odir.csv'\n",
        "    csv_augmented_file = r'ground_truth/odir_augmented.csv'\n",
        "    training_file = r'ground_truth/testing_default_value.csv'\n",
        "    logger.debug('Generating npy files')\n",
        "    generator = NumpyDataGenerator(training_path, testing_path, csv_file, training_file, augmented_path,\n",
        "                                   csv_augmented_file)\n",
        "\n",
        "    # Generate testing file\n",
        "    generator.npy_testing_files('odir_testing_challenge' + '_' + str(image_width), 'odir_testing_labels_challenge' + '_' + str(image_width))\n",
        "\n",
        "    # Generate training file\n",
        "    generator.npy_training_files('odir_training', 'odir_training_labels')\n",
        "    generator.npy_training_files_split(1000, 'odir_training',\n",
        "    'odir_training_labels', 'odir_testing', 'odir_testing_labels')\n",
        "\n",
        "    generator.npy_training_files_split_all(400, 'odir_training' + '_' + str(image_width),\n",
        "                                           'odir_training_labels' + '_' + str(image_width),\n",
        "                                           'odir_testing' + '_' + str(image_width),\n",
        "                                           'odir_testing_labels' + '_' + str(image_width),\n",
        "                                           True)\n",
        "    end = time.time()\n",
        "    logger.debug('Training Records ' + str(generator.total_records_training))\n",
        "    logger.debug('Testing Records ' + str(generator.total_records_testing))\n",
        "    logger.debug('All Done in ' + str(end - start) + ' seconds')"
      ],
      "outputs": [],
      "metadata": {
        "id": "cCwyQJAnkqNq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# create logger\n",
        "logging.config.fileConfig('logging.conf')\n",
        "logger = logging.getLogger('odir')"
      ],
      "outputs": [],
      "metadata": {
        "id": "z45howpPk8qu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# app.run(main)\n",
        "main()"
      ],
      "outputs": [],
      "metadata": {
        "id": "ah_4GdJYlDSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inception v3 model"
      ],
      "metadata": {
        "id": "R45e13hd-pqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required Functions"
      ],
      "metadata": {
        "id": "aCesUE3S2Mw1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# odir_advance_plotting.py\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "\n",
        "\n",
        "class Plotter:\n",
        "    def __init__(self, class_names):\n",
        "        self.class_names = class_names\n",
        "\n",
        "    def plot_metrics(self, history, test_run, index):\n",
        "        metrics2 = ['loss', 'auc', 'precision', 'recall']\n",
        "        for n, metric in enumerate(metrics2):\n",
        "            name = metric.replace(\"_\", \" \").capitalize()\n",
        "            plt.subplot(2, 2, n + 1)\n",
        "            plt.plot(history.epoch, history.history[metric], color='green', label='Train')\n",
        "            plt.plot(history.epoch, history.history['val_' + metric], color='green', linestyle=\"--\", label='Val')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel(name)\n",
        "            if metric == 'loss':\n",
        "                plt.ylim([0, plt.ylim()[1]])\n",
        "            elif metric == 'auc':\n",
        "                plt.ylim([0, 1])\n",
        "            else:\n",
        "                plt.ylim([0, 1])\n",
        "\n",
        "            plt.legend()\n",
        "\n",
        "        #fig_manager = plt.get_current_fig_manager()\n",
        "        #fig_manager.full_screen_toggle()\n",
        "        plt.subplots_adjust(top=0.97, bottom=0.09, left=0.10, right=0.96, hspace=0.25, wspace=0.26)\n",
        "        plt.savefig(test_run)\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    def plot_input_images(self, x_train, y_train):\n",
        "        plt.figure(figsize=(9, 9))\n",
        "        for i in range(100):\n",
        "            plt.subplot(10, 10, i + 1)\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "            plt.grid(False)\n",
        "            plt.imshow(x_train[i])\n",
        "            classes = \"\"\n",
        "            for j in range(8):\n",
        "                if y_train[i][j] >= 0.5:\n",
        "                    classes = classes + self.class_names[j] + \"\\n\"\n",
        "            plt.xlabel(classes, fontsize=7, color='black', labelpad=1)\n",
        "\n",
        "        plt.subplots_adjust(bottom=0.04, right=0.95, top=0.94, left=0.06, wspace=0.56, hspace=0.17)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_image(self, i, predictions_array, true_label, img):\n",
        "        predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
        "        plt.grid(False)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "\n",
        "        plt.imshow(img)\n",
        "        label_check = [0,0,0,0,0,0,0,0]\n",
        "        ground = \"\"\n",
        "        count_true = 0\n",
        "        predicted_true = 0\n",
        "\n",
        "        for index in range(8):\n",
        "            if true_label[index] >= 0.5:\n",
        "                count_true = count_true + 1\n",
        "                ground = ground + self.class_names[index] + \"\\n\"\n",
        "                label_check[index] = 1\n",
        "            if predictions_array[index] >= 0.5:\n",
        "                predicted_true = predicted_true + 1\n",
        "                label_check[index] = label_check[index] + 1\n",
        "\n",
        "        all_match = True\n",
        "        for index in range(8):\n",
        "            if label_check[index]==1:\n",
        "                all_match = False\n",
        "\n",
        "        if count_true == predicted_true and all_match:\n",
        "            color = 'green'\n",
        "        else:\n",
        "            color = 'red'\n",
        "\n",
        "        first, second, third, i, j, k = self.calculate_3_largest(predictions_array, 8)\n",
        "        prediction = \"{} {:2.0f}% \\n\".format(self.class_names[i], 100 * first)\n",
        "        if second >= 0.5:\n",
        "            prediction = prediction + \"{} {:2.0f}% \\n\".format(self.class_names[j], 100 * second)\n",
        "        if third >= 0.5:\n",
        "            prediction = prediction + \"{} {:2.0f}% \\n\".format(self.class_names[k], 100 * third)\n",
        "        plt.xlabel(\"Predicted: {} Ground Truth: {}\".format(prediction, ground), color=color)\n",
        "\n",
        "    def plot_accuracy(self, history, new_folder):\n",
        "        # Hide meanwhile for now\n",
        "        plt.plot(history.history['accuracy'], label='accuracy')\n",
        "        plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.savefig(new_folder)\n",
        "        plt.show()\n",
        "\n",
        "    def calculate_3_largest(self, arr, arr_size):\n",
        "        if arr_size < 3:\n",
        "            print(\" Invalid Input \")\n",
        "            return\n",
        "\n",
        "        third = first = second = -sys.maxsize\n",
        "        index_1 = 0\n",
        "        index_2 = 0\n",
        "        index_3 = 0\n",
        "\n",
        "        for i in range(0, arr_size):\n",
        "            if arr[i] > first:\n",
        "                third = second\n",
        "                second = first\n",
        "                first = arr[i]\n",
        "            elif arr[i] > second:\n",
        "                third = second\n",
        "                second = arr[i]\n",
        "            elif arr[i] > third:\n",
        "                third = arr[i]\n",
        "\n",
        "        for i in range(0, arr_size):\n",
        "            if arr[i] == first:\n",
        "                index_1 = i\n",
        "        for i in range(0, arr_size):\n",
        "            if arr[i] == second and i != index_1:\n",
        "                index_2 = i\n",
        "        for i in range(0, arr_size):\n",
        "            if arr[i] == third and i != index_1 and i!= index_2:\n",
        "                index_3 = i\n",
        "        return first, second, third, index_1, index_2, index_3\n",
        "\n",
        "    def plot_value_array(self, i, predictions_array, true_label):\n",
        "        predictions_array, true_label = predictions_array[i], true_label[i]\n",
        "        plt.grid(False)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        bar_plot = plt.bar(range(8), predictions_array, color=\"#777777\")\n",
        "        plt.xticks(range(8), ('N', 'D', 'G', 'C', 'A', 'H', 'M', 'O'))\n",
        "        plt.ylim([0, 1])\n",
        "\n",
        "        for j in range(8):\n",
        "            if true_label[j] >= 0.5:\n",
        "                bar_plot[j].set_color('green')\n",
        "\n",
        "        for j in range(8):\n",
        "            if predictions_array[j] >= 0.5 and true_label[j] < 0.5:\n",
        "                bar_plot[j].set_color('red')\n",
        "\n",
        "        def bar_label(rects):\n",
        "            for rect in rects:\n",
        "                height = rect.get_height()\n",
        "                value = height * 100\n",
        "                if value > 1:\n",
        "                    plt.annotate('{:2.0f}%'.format(value),\n",
        "                                 xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                                 xytext=(0, 3),  # 3 points vertical offset\n",
        "                                 textcoords=\"offset points\",\n",
        "                                 ha='center', va='bottom')\n",
        "\n",
        "        bar_label(bar_plot)\n",
        "\n",
        "    def ensure_test_prediction_exists(self, predictions):\n",
        "        exists = False\n",
        "        for j in range(8):\n",
        "            if predictions[j] >= 0.5:\n",
        "                exists = True\n",
        "        return exists\n",
        "\n",
        "    def plot_output(self, test_predictions_baseline, y_test, x_test_drawing, test_run):\n",
        "        mpl.rcParams[\"font.size\"] = 7\n",
        "        num_rows = 5\n",
        "        num_cols = 5\n",
        "        num_images = num_rows * num_cols\n",
        "        plt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n",
        "        j = 0\n",
        "        i = 0\n",
        "        while j < num_images:\n",
        "            if self.ensure_test_prediction_exists(test_predictions_baseline[i]):\n",
        "                plt.subplot(num_rows, 2 * num_cols, 2 * j + 1)\n",
        "                self.plot_image(i, test_predictions_baseline, y_test, x_test_drawing)\n",
        "                plt.subplot(num_rows, 2 * num_cols, 2 * j + 2)\n",
        "                self.plot_value_array(i, test_predictions_baseline, y_test)\n",
        "                j = j + 1\n",
        "            i = i + 1\n",
        "            if i > 400:\n",
        "                break\n",
        "\n",
        "        plt.subplots_adjust(bottom=0.08, right=0.95, top=0.94, left=0.05, wspace=0.11, hspace=0.56)\n",
        "        plt.savefig(test_run)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_output_single(self, i, test_predictions_baseline, y_test, x_test_drawing):\n",
        "        plt.figure(figsize=(6, 3))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        self.plot_image(i, test_predictions_baseline, y_test, x_test_drawing)\n",
        "        plt.subplot(1, 2, 2)\n",
        "        self.plot_value_array(i, test_predictions_baseline, y_test)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_confusion_matrix(self, y_true, y_pred, classes, normalize=False, title=None, cmap=plt.cm.Blues):\n",
        "        \"\"\"\n",
        "        This function prints and plots the confusion matrix.\n",
        "        Normalization can be applied by setting `normalize=True`.\n",
        "        \"\"\"\n",
        "        if not title:\n",
        "            if normalize:\n",
        "                title = 'Normalized confusion matrix'\n",
        "            else:\n",
        "                title = 'Confusion matrix, without normalization'\n",
        "\n",
        "        # Compute confusion matrix\n",
        "        cm = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "        # Only use the labels that appear in the data\n",
        "        if normalize:\n",
        "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "            print(\"Normalized confusion matrix\")\n",
        "        else:\n",
        "            print('Confusion matrix, without normalization')\n",
        "\n",
        "        print(cm)\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "        ax.figure.colorbar(im, ax=ax)\n",
        "        # We want to show all ticks...\n",
        "        ax.set(xticks=np.arange(cm.shape[1]),\n",
        "               yticks=np.arange(cm.shape[0]),\n",
        "               # ... and label them with the respective list entries\n",
        "               # xticklabels=classes, yticklabels=classes,\n",
        "               title=title,\n",
        "               ylabel='True label',\n",
        "               xlabel='Predicted label')\n",
        "        ax.set_ylim(8.0, -1.0)\n",
        "        # Rotate the tick labels and set their alignment.\n",
        "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "                 rotation_mode=\"anchor\")\n",
        "\n",
        "        # Loop over data dimensions and create text annotations.\n",
        "        fmt = '.2f' if normalize else 'd'\n",
        "        thresh = cm.max() / 2.\n",
        "        for i in range(cm.shape[0]):\n",
        "            for j in range(cm.shape[1]):\n",
        "                ax.text(j, i, format(cm[i, j], fmt),\n",
        "                        ha=\"center\", va=\"center\",\n",
        "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        fig.tight_layout()\n",
        "        return ax\n",
        "\n",
        "    def print_normalized_confusion_matrix(self, y_test, test_predictions_baseline):\n",
        "        np.set_printoptions(precision=2)\n",
        "\n",
        "        # Plot non-normalized confusion matrix\n",
        "        self.plot_confusion_matrix(y_test, test_predictions_baseline, classes=self.class_names,\n",
        "                                   title='Confusion matrix, without normalization')\n",
        "\n",
        "        # Plot normalized confusion matrix\n",
        "        self.plot_confusion_matrix(y_test, test_predictions_baseline, classes=self.class_names, normalize=True,\n",
        "                                   title='Normalized confusion matrix')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def plot_confusion_matrix_generic(self, labels2, predictions, test_run, p=0.5):\n",
        "        cm = confusion_matrix(labels2.argmax(axis=1), predictions.argmax(axis=1))\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        ax = sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "        ax.set_ylim(8.0, -1.0)\n",
        "        plt.title('Confusion matrix')\n",
        "        plt.ylabel('Actual label')\n",
        "        plt.xlabel('Predicted label')\n",
        "        plt.savefig(test_run)\n",
        "        plt.subplots_adjust(top=0.94, bottom=0.11, left=0.12, right=1.00, hspace=0.20, wspace=0.18)\n",
        "        plt.show()\n",
        "        plt.close()"
      ],
      "outputs": [],
      "metadata": {
        "id": "SpNAaudg2GdU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# odir_kappa_score.py\n",
        "import csv\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "class FinalScore:\n",
        "    def __init__(self, new_folder):\n",
        "        self.new_folder = new_folder\n",
        "\n",
        "\n",
        "    def odir_metrics(self, gt_data, pr_data):\n",
        "        th = 0.5\n",
        "        gt = gt_data.flatten()\n",
        "        pr = pr_data.flatten()\n",
        "        kappa = metrics.cohen_kappa_score(gt, pr > th)\n",
        "        f1 = metrics.f1_score(gt, pr > th, average='micro')\n",
        "        auc = metrics.roc_auc_score(gt, pr)\n",
        "        final_score = (kappa + f1 + auc) / 3.0\n",
        "        return kappa, f1, auc, final_score\n",
        "\n",
        "    def import_data(self, filepath):\n",
        "        with open(filepath, 'r') as f:\n",
        "            reader = csv.reader(f)\n",
        "            header = next(reader)\n",
        "            pr_data = [[int(row[0])] + list(map(float, row[1:])) for row in reader]\n",
        "        pr_data = np.array(pr_data)\n",
        "        return pr_data\n",
        "\n",
        "    def output(self):\n",
        "        gt_data = self.import_data(os.path.join(self.new_folder, 'odir_ground_truth.csv'))\n",
        "        pr_data = self.import_data(os.path.join(self.new_folder, 'odir_predictions.csv'))\n",
        "        kappa, f1, auc, final_score = self.odir_metrics(gt_data[:, 1:], pr_data[:, 1:])\n",
        "        print(\"Kappa score:\", kappa)\n",
        "        print(\"F-1 score:\", f1)\n",
        "        print(\"AUC value:\", auc)\n",
        "        print(\"Final Score:\", final_score)"
      ],
      "outputs": [],
      "metadata": {
        "id": "7oDuvbCX215-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# odir_predictions_writer.py\n",
        "import csv\n",
        "import os\n",
        "\n",
        "\n",
        "class Prediction:\n",
        "    def __init__(self, prediction, num_images_test, folder = \"\"):\n",
        "        self.prediction = prediction\n",
        "        self.num_images_test = num_images_test\n",
        "        self.folder = folder\n",
        "\n",
        "    def save(self):\n",
        "        \"\"\"Generate a CSV that contains the output of all the classes.\n",
        "        Args:\n",
        "          No arguments are required.\n",
        "        Returns:\n",
        "          File with the output\n",
        "        \"\"\"\n",
        "        # The process here is to generate a CSV file with the content of the data annotations file\n",
        "        # and also the total of labels per eye. This will help us later to process the images\n",
        "        if self.folder != \"\":\n",
        "            folder_to_save = os.path.join(self.folder, 'predictions.csv')\n",
        "        else:\n",
        "            folder_to_save = 'predictions.csv'\n",
        "        with open(folder_to_save, 'w', newline='') as csv_file:\n",
        "            file_writer = csv.writer(csv_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "            file_writer.writerow(['ID', 'Normal', 'Diabetes', 'Glaucoma', 'Cataract', 'AMD', 'Hypertension', 'Myopia', 'Others'])\n",
        "            count = 0\n",
        "            for sub in self.prediction:\n",
        "                normal = sub[0]\n",
        "                diabetes = sub[1]\n",
        "                glaucoma = sub[2]\n",
        "                cataract = sub[3]\n",
        "                amd = sub[4]\n",
        "                hypertension = sub[5]\n",
        "                myopia = sub[6]\n",
        "                others = sub[7]\n",
        "                file_writer.writerow([count, normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
        "                count = count + 1\n",
        "\n",
        "    def save_all(self, y_test):\n",
        "        \"\"\"Generate a CSV that contains the output of all the classes.\n",
        "        Args:\n",
        "          No arguments are required.\n",
        "        Returns:\n",
        "          File with the output\n",
        "        \"\"\"\n",
        "        # The process here is to generate a CSV file with the content of the data annotations file\n",
        "        # and also the total of labels per eye. This will help us later to process the images\n",
        "        if self.folder != \"\":\n",
        "            folder_to_save = os.path.join(self.folder, 'odir_predictions.csv')\n",
        "        else:\n",
        "            folder_to_save = 'odir_predictions.csv'\n",
        "        with open(folder_to_save, 'w', newline='') as csv_file:\n",
        "            file_writer = csv.writer(csv_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "            file_writer.writerow(['ID', 'N', 'D', 'G', 'C', 'A', 'H', 'M', 'O'])\n",
        "            count = 0\n",
        "            for i in range(self.num_images_test):\n",
        "                normal = self.prediction[i][0]\n",
        "                diabetes = self.prediction[i][1]\n",
        "                glaucoma = self.prediction[i][2]\n",
        "                cataract = self.prediction[i][3]\n",
        "                amd = self.prediction[i][4]\n",
        "                hypertension = self.prediction[i][5]\n",
        "                myopia = self.prediction[i][6]\n",
        "                others = self.prediction[i][7]\n",
        "                file_writer.writerow([count, normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
        "                count = count + 1\n",
        "\n",
        "        if self.folder != \"\":\n",
        "            folder_to_save = os.path.join(self.folder, 'odir_ground_truth.csv')\n",
        "        else:\n",
        "            folder_to_save = 'odir_ground_truth.csv'\n",
        "        with open(folder_to_save, 'w', newline='') as csv_file:\n",
        "            file_writer = csv.writer(csv_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "            file_writer.writerow(['ID', 'N', 'D', 'G', 'C', 'A', 'H', 'M', 'O'])\n",
        "            count = 0\n",
        "            for i in range(self.num_images_test):\n",
        "                normal2 = y_test[i][0]\n",
        "                diabetes2 = y_test[i][1]\n",
        "                glaucoma2 = y_test[i][2]\n",
        "                cataract2 = y_test[i][3]\n",
        "                amd2 = y_test[i][4]\n",
        "                hypertension2 = y_test[i][5]\n",
        "                myopia2 = y_test[i][6]\n",
        "                others2 = y_test[i][7]\n",
        "\n",
        "                file_writer.writerow([count, normal2, diabetes2, glaucoma2, cataract2, amd2, hypertension2, myopia2, others2])\n",
        "                count = count + 1"
      ],
      "outputs": [],
      "metadata": {
        "id": "-fXWSP5g3YiE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# odir.py\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_data(image_size, index = 0, challenge = 0):\n",
        "    \"\"\"Loads the ODIR dataset.\n",
        "    Returns:\n",
        "      Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
        "    \"\"\"\n",
        "\n",
        "    if index == 0:\n",
        "        x_train = np.load('odir_training' + '_' + str(image_size) + '.npy')\n",
        "        y_train = np.load('odir_training_labels' + '_' + str(image_size) + '.npy')\n",
        "    else:\n",
        "        x_train = np.load('odir_training' + '_' + str(image_size) + '_' + str(index) + '.npy')\n",
        "        y_train = np.load('odir_training_labels' + '_' + str(image_size) + '_' + str(index) + '.npy')\n",
        "\n",
        "    if challenge == 0:\n",
        "        x_test = np.load('odir_testing'+'_' + str(image_size)+'.npy')\n",
        "        y_test = np.load('odir_testing_labels'+'_' + str(image_size)+'.npy')\n",
        "    else:\n",
        "        x_test = np.load('odir_testing_challenge'+'_' + str(image_size)+'.npy')\n",
        "        y_test = np.load('odir_testing_labels_challenge'+'_' + str(image_size)+'.npy')\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "outputs": [],
      "metadata": {
        "id": "iVWqcjw75Bpg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def normalize_vgg16(training, testing):\n",
        "    training, testing = training / 1.0, testing / 1.0\n",
        "\n",
        "    # training[:, :, 0] -= 103.939\n",
        "    # training[:, :, 1] -= 116.779\n",
        "    # training[:, :, 2] -= 123.68\n",
        "    # training = training.transpose((1, 0, 2))\n",
        "    # training = np.expand_dims(training, axis=0)\n",
        "    #\n",
        "    # testing[:, :, 0] -= 103.939\n",
        "    # testing[:, :, 1] -= 116.779\n",
        "    # testing[:, :, 2] -= 123.68\n",
        "    # testing = testing.transpose((1, 0, 2))\n",
        "    # testing = np.expand_dims(testing, axis=0)\n",
        "\n",
        "    training = training[..., ::-1]\n",
        "    testing = testing[..., ::-1]\n",
        "    mean = [103.939, 116.779, 123.68]\n",
        "    training[..., 0] -= mean[0]\n",
        "    training[..., 1] -= mean[1]\n",
        "    training[..., 2] -= mean[2]\n",
        "    testing[..., 0] -= mean[0]\n",
        "    testing[..., 1] -= mean[1]\n",
        "    testing[..., 2] -= mean[2]\n",
        "\n",
        "    #training = (training - training.mean()) / training.std()\n",
        "    #testing = (testing - testing.mean()) / testing.std()\n",
        "    return training, testing"
      ],
      "outputs": [],
      "metadata": {
        "id": "J86wUpoU28o8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Model"
      ],
      "metadata": {
        "id": "x358Ascp0qDs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import inception_v3\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
        "import secrets\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "# batch_size = 32\n",
        "batch_size = 16\n",
        "num_classes = 8\n",
        "# epochs = 100\n",
        "epochs = 2\n",
        "patience = 5\n",
        "\n",
        "# class_weight = {0: 1.,\n",
        "#                 1: 1.583802025,\n",
        "#                 2: 8.996805112,\n",
        "#                 3: 10.24,\n",
        "#                 4: 10.05714286,\n",
        "#                 5: 1.,\n",
        "#                 6: 1.,\n",
        "#                 7: 2.505338078}\n",
        "\n",
        "token = secrets.token_hex(16)\n",
        "folder = r'/content/drive/MyDrive/EyeDisease_Bahaloo/OcularDisease/Inception_basic_outputs'\n",
        "\n",
        "new_folder = os.path.join(folder, token)\n",
        "\n",
        "if not os.path.exists(new_folder):\n",
        "    os.makedirs(new_folder)\n",
        "\n",
        "base_model = inception_v3.InceptionV3\n",
        "\n",
        "base_model = base_model(weights='imagenet', include_top=False)\n",
        "\n",
        "# Comment this out if you want to train all layers\n",
        "#for layer in base_model.layers:\n",
        "#    layer.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(num_classes, activation='sigmoid')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "model.summary()\n",
        "\n",
        "tf.keras.utils.plot_model(model, to_file=os.path.join(new_folder, 'model_inception_v3.png'), show_shapes=True, show_layer_names=True)\n",
        "\n",
        "defined_metrics = [\n",
        "    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "    tf.keras.metrics.Precision(name='precision'),\n",
        "    tf.keras.metrics.Recall(name='recall'),\n",
        "    tf.keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "\n",
        "# Adam Optimizer Example\n",
        "# model.compile(loss='binary_crossentropy',\n",
        "#               optimizer=Adam(lr=0.001),\n",
        "#               metrics=defined_metrics)\n",
        "\n",
        "# RMSProp Optimizer Example\n",
        "# model.compile(loss='binary_crossentropy',\n",
        "#               optimizer='rmsprop',\n",
        "#               metrics=defined_metrics)\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "print('Configuration Start -------------------------')\n",
        "print(sgd.get_config())\n",
        "print('Configuration End -------------------------')\n",
        "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=defined_metrics)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_data(224, index=1)\n",
        "\n",
        "x_test_drawing = x_test\n",
        "\n",
        "x_train = inception_v3.preprocess_input(x_train)\n",
        "x_test = inception_v3.preprocess_input(x_test)\n",
        "\n",
        "class_names = ['Normal', 'Diabetes', 'Glaucoma', 'Cataract', 'AMD', 'Hypertension', 'Myopia', 'Others']\n",
        "\n",
        "# plot data input\n",
        "plotter = Plotter(class_names)\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min', verbose=1)\n",
        "\n",
        "#class_weight = class_weight.compute_class_weight('balanced', np.unique(x_train), x_train)\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True, #class_weight= class_weight,\n",
        "                    validation_data=(x_test, y_test), callbacks=[callback])\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZzE1A_dC-r_j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"saving weights\")\n",
        "model.save(os.path.join(new_folder, 'model_weights.h5'))\n",
        "\n",
        "print(\"plotting metrics\")\n",
        "plotter.plot_metrics(history, os.path.join(new_folder, 'plot1.png'), 2)\n",
        "\n",
        "print(\"plotting accuracy\")\n",
        "plotter.plot_accuracy(history, os.path.join(new_folder, 'plot2.png'))\n",
        "\n",
        "print(\"display the content of the model\")\n",
        "baseline_results = model.evaluate(x_test, y_test, verbose=2)\n",
        "for name, value in zip(model.metrics_names, baseline_results):\n",
        "    print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "# test a prediction\n",
        "test_predictions_baseline = model.predict(x_test)\n",
        "print(\"plotting confusion matrix\")\n",
        "plotter.plot_confusion_matrix_generic(y_test, test_predictions_baseline, os.path.join(new_folder, 'plot3.png'), 0)\n",
        "\n",
        "# save the predictions\n",
        "prediction_writer = Prediction(test_predictions_baseline, 400, new_folder)\n",
        "prediction_writer.save()\n",
        "prediction_writer.save_all(y_test)\n",
        "\n",
        "# show the final score\n",
        "score = FinalScore(new_folder)\n",
        "score.output()\n",
        "\n",
        "# plot output results\n",
        "plotter.plot_output(test_predictions_baseline, y_test, x_test_drawing, os.path.join(new_folder, 'plot4.png'))"
      ],
      "outputs": [],
      "metadata": {
        "id": "77PI7OSA7q2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enhanced Model"
      ],
      "metadata": {
        "id": "ClY3s8X60uR1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "from collections import Sequence\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications import resnet50, inception_v3, vgg16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
        "import numpy as np\n",
        "import secrets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# batch_size = 32\n",
        "batch_size = 16\n",
        "num_classes = 8\n",
        "# epochs = 30\n",
        "epochs = 1\n",
        "\n",
        "\n",
        "class Generator(Sequence):\n",
        "    # Class is a dataset wrapper for better training performance\n",
        "    def __init__(self, x_set, y_set, batch_size=256):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "        self.indices = np.arange(self.x.shape[0])\n",
        "\n",
        "    def __len__(self):\n",
        "        return np.math.ceil(self.x.shape[0] / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_x = self.x[inds]\n",
        "        batch_y = self.y[inds]\n",
        "        return batch_x, batch_y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "def generator(train_a, labels_a, train_b, labels_b):\n",
        "    while True:\n",
        "        for i in range(len(train_a)):\n",
        "            yield train_a[i].reshape(1, 224, 224, 3), labels_a[i].reshape(1, 8)\n",
        "        for i in range(len(train_b)):\n",
        "            yield train_b[i].reshape(1, 224, 224, 3), labels_b[i].reshape(1, 8)\n",
        "\n",
        "def generator_validation(test, labels):\n",
        "    while True:\n",
        "        for i in range(len(test)):\n",
        "            yield test[i].reshape(1, 224, 224, 3), labels[i].reshape(1, 8)\n",
        "\n",
        "token = secrets.token_hex(16)\n",
        "folder = r'/content/drive/MyDrive/EyeDisease_Bahaloo/OcularDisease/Inception_enhanced_outputs'\n",
        "\n",
        "newfolder = os.path.join(folder, token)\n",
        "if not os.path.exists(newfolder):\n",
        "    os.makedirs(newfolder)\n",
        "\n",
        "base_model = inception_v3.InceptionV3\n",
        "\n",
        "base_model = base_model(weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(num_classes, activation='sigmoid')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "# model.summary()\n",
        "\n",
        "tf.keras.utils.plot_model(model, to_file=os.path.join(newfolder, 'model_inception_v3.png'), show_shapes=True, show_layer_names=True)\n",
        "\n",
        "\n",
        "#for layer in base_model.layers:\n",
        "#    layer.trainable = False\n",
        "\n",
        "defined_metrics = [\n",
        "    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "    tf.keras.metrics.Precision(name='precision'),\n",
        "    tf.keras.metrics.Recall(name='recall'),\n",
        "    tf.keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(lr=0.001),\n",
        "              metrics=defined_metrics)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_data(224, 1)\n",
        "(x_train2, y_train2), (x_test, y_test) = load_data(224, 2)\n",
        "\n",
        "x_test_drawing = x_test"
      ],
      "outputs": [],
      "metadata": {
        "id": "4DJqaVal0trT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# __________these 3 lines below commented because RAM crashed with them uncommented____________\n",
        "# x_train = inception_v3.preprocess_input(x_train)\n",
        "# x_train2 = inception_v3.preprocess_input(x_train2)\n",
        "# x_test = inception_v3.preprocess_input(x_test)\n",
        "\n",
        "#print(model.evaluate(x_train, y_train, batch_size=batch_size, verbose=0))\n",
        "class_names = ['Normal', 'Diabetes', 'Glaucoma', 'Cataract', 'AMD', 'Hypertension', 'Myopia', 'Others']\n",
        "\n",
        "# plot data input\n",
        "plotter = Plotter(class_names)\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1)\n",
        "\n",
        "# class_weight = {0: 1.,\n",
        "#                     1: 1.583802025,\n",
        "#                     2: 8.996805112,\n",
        "#                     3: 10.24,\n",
        "#                     4: 10.05714286,\n",
        "#                     5: 14.66666667,\n",
        "#                     6: 10.7480916,\n",
        "#                     7: 2.505338078} , class_weight=class_weight"
      ],
      "outputs": [],
      "metadata": {
        "id": "Sje2BPB-5L7u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "          epochs=1,\n",
        "          batch_size=batch_size,\n",
        "          shuffle=True,\n",
        "          validation_data=(x_test, y_test), callbacks=[callback])"
      ],
      "outputs": [],
      "metadata": {
        "id": "XEoLFLWp4ElU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# train_datagen = Generator(x_train, y_train, batch_size)\n",
        "# # With Data Augmentation\n",
        "# history = model.fit_generator(generator=generator(x_train, y_train, x_train2, y_train2), steps_per_epoch=len(x_train),\n",
        "#                                epochs=epochs, verbose=1, callbacks=[callback], validation_data=generator_validation(x_test, y_test),\n",
        "#                               validation_steps=len(x_test), shuffle=False )"
      ],
      "outputs": [],
      "metadata": {
        "id": "EyWt5i-dw6IB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"saving\")\n",
        "model.save(os.path.join(newfolder, 'model_weights.h5'))\n",
        "\n",
        "print(\"plotting\")\n",
        "plotter.plot_metrics(history, os.path.join(newfolder, 'plot1.png'), 2)\n",
        "\n",
        "# Hide meanwhile for now\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.savefig(os.path.join(newfolder, 'plot2.png'))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# display the content of the model\n",
        "baseline_results = model.evaluate(x_test, y_test, verbose=2)\n",
        "for name, value in zip(model.metrics_names, baseline_results):\n",
        "    print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "# test a prediction\n",
        "test_predictions_baseline = model.predict(x_test)\n",
        "plotter.plot_confusion_matrix_generic(y_test, test_predictions_baseline, os.path.join(newfolder, 'plot3.png'), 0)\n",
        "\n",
        "# save the predictions\n",
        "prediction_writer = Prediction(test_predictions_baseline, 400, newfolder)\n",
        "prediction_writer.save()\n",
        "prediction_writer.save_all(y_test)\n",
        "\n",
        "# show the final score\n",
        "score = FinalScore(newfolder)\n",
        "score.output()\n",
        "\n",
        "# plot output results\n",
        "plotter.plot_output(test_predictions_baseline, y_test, x_test_drawing, os.path.join(newfolder, 'plot4.png'))"
      ],
      "outputs": [],
      "metadata": {
        "id": "8IBjHnv4w27b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "UPRUvGvv0x4t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import logging.config\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from absl import app\n",
        "# from keras.applications.inception_v3 import keras_utils\n",
        "from tensorflow.keras.applications import inception_v3\n",
        "\n",
        "# def main(argv):\n",
        "def main():\n",
        "    print(tf.version.VERSION)\n",
        "    image_size = 224\n",
        "    test_run = 'zCSA'\n",
        "    new_folder = r'/content/drive/MyDrive/EyeDisease_Bahaloo/OcularDisease/Inception_basic_outputs/682e72d08d5e4829d4d4250b545e29d2'\n",
        "    # load the data\n",
        "    (x_train, y_train), (x_test, y_test) = load_data(image_size, 1)\n",
        "\n",
        "    class_names = ['Normal', 'Diabetes', 'Glaucoma', 'Cataract', 'AMD', 'Hypertension', 'Myopia', 'Others']\n",
        "\n",
        "    # plot data input\n",
        "    plotter = Plotter(class_names)\n",
        "    plotter.plot_input_images(x_test, y_test)\n",
        "\n",
        "    x_test_drawing = x_test\n",
        "\n",
        "    # normalize input based on model\n",
        "    #____________below commented_______________\n",
        "    # x_test = inception_v3.preprocess_input(x_test)\n",
        "\n",
        "    # load one of the test runs\n",
        "    model = tf.keras.models.load_model(os.path.join(new_folder , 'model_weights.h5'))\n",
        "    model.summary()\n",
        "\n",
        "    # display the content of the model\n",
        "    baseline_results = model.evaluate(x_test, y_test, verbose=2)\n",
        "    for name, value in zip(model.metrics_names, baseline_results):\n",
        "        print(name, ': ', value)\n",
        "    print()\n",
        "\n",
        "    # test a prediction\n",
        "    test_predictions_baseline = model.predict(x_test)\n",
        "    plotter.plot_confusion_matrix_generic(y_test, test_predictions_baseline, test_run, 0)\n",
        "\n",
        "    # save the predictions\n",
        "    prediction_writer = Prediction(test_predictions_baseline, 400)\n",
        "    prediction_writer.save()\n",
        "    prediction_writer.save_all(y_test)\n",
        "\n",
        "    # show the final score\n",
        "    score = FinalScore(new_folder)\n",
        "    score.output()\n",
        "\n",
        "    # plot output results\n",
        "    plotter.plot_output(test_predictions_baseline, y_test, x_test_drawing, test_run)"
      ],
      "outputs": [],
      "metadata": {
        "id": "OV0HIQ1P-sli"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# create logger\n",
        "logging.config.fileConfig('logging.conf')\n",
        "logger = logging.getLogger('odir')\n",
        "# app.run(main)\n",
        "main()"
      ],
      "outputs": [],
      "metadata": {
        "id": "MoPZrc3v-sjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG16"
      ],
      "metadata": {
        "id": "ho_CfYlpUZ7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required Functions"
      ],
      "metadata": {
        "id": "kfYV93pSVtPx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from tensorflow.keras import models, layers\n",
        "from abc import abstractmethod\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class ModelBase:\n",
        "    def __init__(self, input_shape, metrics):\n",
        "        self.input_shape = input_shape\n",
        "        self.metrics = metrics\n",
        "\n",
        "    def show_summary(self, model):\n",
        "        model.summary()\n",
        "\n",
        "    def plot_summary(self, model, file_name):\n",
        "        tf.keras.utils.plot_model(model, to_file=file_name, show_shapes=True, show_layer_names=True)\n",
        "\n",
        "    @abstractmethod\n",
        "    def compile(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class Advanced(ModelBase):\n",
        "\n",
        "    def compile(self):\n",
        "        model = models.Sequential()\n",
        "        model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=self.input_shape))\n",
        "        model.add(layers.MaxPooling2D((2, 2)))\n",
        "        model.add(layers.Dropout(0.2))\n",
        "        model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "        model.add(layers.MaxPooling2D((2, 2)))\n",
        "        model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "        model.add(layers.Dropout(0.2))\n",
        "        model.add(layers.Flatten())\n",
        "        model.add(layers.Dense(64, activation='relu'))\n",
        "        model.add(layers.Dense(8, activation='sigmoid'))\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=self.metrics)\n",
        "        self.show_summary(model)\n",
        "        self.plot_summary(model, 'model_advanced.png')\n",
        "        return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "cK4WIKOdWDqe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.python.keras import Input\n",
        "\n",
        "class InceptionV1(ModelBase):\n",
        "\n",
        "    def compile(self):\n",
        "        input_img = Input(shape=self.input_shape)\n",
        "        layer_1 = Conv2D(10, (1, 1), padding='same', activation='relu')(input_img)\n",
        "        layer_1 = Conv2D(10, (3, 3), padding='same', activation='relu')(layer_1)\n",
        "\n",
        "        layer_2 = Conv2D(10, (1, 1), padding='same', activation='relu')(input_img)\n",
        "        layer_2 = Conv2D(10, (5, 5), padding='same', activation='relu')(layer_2)\n",
        "\n",
        "        layer_3 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_img)\n",
        "        layer_3 = Conv2D(10, (1, 1), padding='same', activation='relu')(layer_3)\n",
        "\n",
        "        mid_1 = tensorflow.keras.layers.concatenate([layer_1, layer_2, layer_3], axis=3)\n",
        "        flat_1 = Flatten()(mid_1)\n",
        "\n",
        "        dense_1 = Dense(1200, activation='relu')(flat_1)\n",
        "        dense_2 = Dense(600, activation='relu')(dense_1)\n",
        "        dense_3 = Dense(150, activation='relu')(dense_2)\n",
        "        output = Dense(8, activation='sigmoid')(dense_3)\n",
        "        model = Model([input_img], output)\n",
        "\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=self.metrics)\n",
        "\n",
        "        self.show_summary(model)\n",
        "        self.plot_summary(model, 'model_inception_v1.png')\n",
        "\n",
        "        return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "zzhkGF-SWfqU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "class Vgg16(ModelBase):\n",
        "\n",
        "    def compile(self):\n",
        "        x = models.Sequential()\n",
        "        trainable = False\n",
        "        # Block 1\n",
        "        layer = layers.Conv2D(input_shape=self.input_shape, filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.MaxPooling2D((2, 2), strides=(2, 2))\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "\n",
        "        # Block 2\n",
        "        layer = layers.Conv2D(128, kernel_size=(3,3),padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Conv2D(128, kernel_size=(3,3),padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.MaxPooling2D((2, 2), strides=(2, 2))\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "\n",
        "        # Block 3\n",
        "        layer = layers.Conv2D(256, kernel_size=(3,3),padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Conv2D(256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Conv2D(256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.MaxPooling2D((2, 2), strides=(2, 2))\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "\n",
        "        # Block 4\n",
        "        layer = layers.Conv2D(512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Conv2D(512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Conv2D(512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.MaxPooling2D((2, 2), strides=(2, 2))\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "\n",
        "        # Block 5\n",
        "        layer = layers.Conv2D(512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Conv2D(512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Conv2D(512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.MaxPooling2D((2, 2), strides=(2, 2))\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "\n",
        "        layer = layers.Flatten()\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Dense(4096, activation='relu')\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Dense(4096, activation='relu')\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Dense(1000, activation='softmax')\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "\n",
        "        # Transfer learning, load previous weights\n",
        "        x.load_weights(r'/content/drive/MyDrive/AIMedic/OcularDisease/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n",
        "\n",
        "        # Remove last layer\n",
        "        x.pop()\n",
        "\n",
        "        # Add new dense layer\n",
        "        x.add(layers.Dense(8, activation='sigmoid'))\n",
        "        #optimizer = tensorflow.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "        sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "        print('Configuration Start -------------------------')\n",
        "        print(sgd.get_config())\n",
        "        print('Configuration End -------------------------')\n",
        "        x.compile(optimizer=sgd, loss='binary_crossentropy', metrics=self.metrics)\n",
        "\n",
        "        self.show_summary(x)\n",
        "        self.plot_summary(x, 'model_vgg16net.png')\n",
        "        return x"
      ],
      "outputs": [],
      "metadata": {
        "id": "jhnw2oRaWsCp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import enum\n",
        "\n",
        "class ModelTypes(enum.Enum):\n",
        "    vgg16 = 1\n",
        "    inception_v1 = 2\n",
        "    advanced_testing = 3\n",
        "    vgg19 = 4\n",
        "\n",
        "\n",
        "class Factory:\n",
        "\n",
        "    def __init__(self, input_shape, metrics):\n",
        "        self.Makers = {\n",
        "            ModelTypes.vgg16: Vgg16(input_shape, metrics),\n",
        "            ModelTypes.inception_v1: InceptionV1(input_shape, metrics),\n",
        "            ModelTypes.advanced_testing: Advanced(input_shape, metrics),\n",
        "            ModelTypes.vgg19: Vgg19(input_shape, metrics)\n",
        "        }\n",
        "\n",
        "    def compile(self, model_type):\n",
        "        return self.Makers[model_type].compile()"
      ],
      "outputs": [],
      "metadata": {
        "id": "RPrNQrIEVssW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "class Vgg19(ModelBase):\n",
        "\n",
        "    def compile(self):\n",
        "        x = models.Sequential()\n",
        "        trainable = False\n",
        "        # Block 1\n",
        "        layer = layers.Conv2D(input_shape=self.input_shape, filters=64, kernel_size=(3, 3), padding=\"same\",\n",
        "                              activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.MaxPooling2D((2, 2), strides=(2, 2))\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "\n",
        "        # Block 2\n",
        "        layer = layers.Conv2D(128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Conv2D(128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.MaxPooling2D((2, 2), strides=(2, 2))\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "\n",
        "        # Block 3\n",
        "        layer = layers.Conv2D(256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Conv2D(256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Conv2D(256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Conv2D(256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.MaxPooling2D((2, 2), strides=(2, 2))\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "\n",
        "        # Block 4\n",
        "        layer = layers.Conv2D(512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Conv2D(512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Conv2D(512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Conv2D(512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.MaxPooling2D((2, 2), strides=(2, 2))\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "\n",
        "        # Block 5\n",
        "        layer = layers.Conv2D(512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Conv2D(512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Conv2D(512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Conv2D(512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.MaxPooling2D((2, 2), strides=(2, 2))\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "\n",
        "        layer = layers.Flatten()\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        layer = layers.Dense(4096, activation='relu')\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        #layer = layers.Dropout(0.5)\n",
        "        #layer.trainable = True\n",
        "        #x.add(layer)\n",
        "        layer = layers.Dense(4096, activation='relu')\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "        #layer = layers.Dropout(0.5)\n",
        "        #layer.trainable = True\n",
        "        #x.add(layer)\n",
        "        layer = layers.Dense(1000, activation='softmax')\n",
        "        layer.trainable = trainable\n",
        "        x.add(layer)\n",
        "\n",
        "        # Transfer learning, load previous weights\n",
        "        x.load_weights(r'/content/drive/MyDrive/AIMedic/OcularDisease/vgg19_weights_tf_dim_ordering_tf_kernels.h5')\n",
        "\n",
        "        # Remove last layer\n",
        "        x.pop()\n",
        "\n",
        "        # Add new dense layer\n",
        "        #x.add(layers.Dropout(0.1))\n",
        "        x.add(layers.Dense(8, activation='sigmoid'))\n",
        "        # optimizer = tensorflow.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "        sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=False)\n",
        "        print('Configuration Start -------------------------')\n",
        "        print(sgd.get_config())\n",
        "        print('Configuration End -------------------------')\n",
        "        x.compile(optimizer=sgd, loss='binary_crossentropy', metrics=self.metrics)\n",
        "\n",
        "        self.show_summary(x)\n",
        "        self.plot_summary(x, 'model_vgg19net.png')\n",
        "        return x"
      ],
      "outputs": [],
      "metadata": {
        "id": "MyeJR9U_g9bT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Model"
      ],
      "metadata": {
        "id": "vLqXL726UlgX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "# os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
        "import secrets\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications import vgg16\n",
        "\n",
        "# batch_size = 32\n",
        "batch_size = 16\n",
        "num_classes = 8\n",
        "# epochs = 50\n",
        "epochs = 1\n",
        "patience = 5\n",
        "\n",
        "token = secrets.token_hex(16)\n",
        "folder = r'/content/drive/MyDrive/AIMedic/OcularDisease/VGG16_basic_outputs'\n",
        "\n",
        "new_folder = os.path.join(folder, token)\n",
        "\n",
        "if not os.path.exists(new_folder):\n",
        "    os.makedirs(new_folder)\n",
        "\n",
        "defined_metrics = [\n",
        "    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "    tf.keras.metrics.Precision(name='precision'),\n",
        "    tf.keras.metrics.Recall(name='recall'),\n",
        "    tf.keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "\n",
        "factory = Factory((224, 224, 3), defined_metrics)\n",
        "model = factory.compile(ModelTypes.vgg16)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_data(224, 1)\n",
        "\n",
        "x_test_drawing = x_test\n",
        "\n",
        "# _______________________below commented________________________\n",
        "# x_train = vgg16.preprocess_input(x_train)\n",
        "# x_test = vgg16.preprocess_input(x_test)\n",
        "\n",
        "class_names = ['Normal', 'Diabetes', 'Glaucoma', 'Cataract', 'AMD', 'Hypertension', 'Myopia', 'Others']\n",
        "\n",
        "# plot data input\n",
        "plotter = Plotter(class_names)\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min', verbose=1)\n",
        "\n",
        "class_weight = {0: 1.,\n",
        "                1: 1.583802025,\n",
        "                2: 8.996805112,\n",
        "                3: 10.24,\n",
        "                4: 10.05714286,\n",
        "                5: 1.,\n",
        "                6: 1.,\n",
        "                7: 2.505338078}"
      ],
      "outputs": [],
      "metadata": {
        "id": "jVMvuDinHZ8P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True, #class_weight = class_weight,\n",
        "                    validation_data=(x_test, y_test), callbacks=[callback])"
      ],
      "outputs": [],
      "metadata": {
        "id": "I0cPdXCSjQqR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"saving\")\n",
        "model.save(os.path.join(new_folder, 'model_weights.h5'))\n",
        "\n",
        "print(\"plotting\")\n",
        "plotter.plot_metrics(history, os.path.join(new_folder, 'plot1.png'), 2)\n",
        "\n",
        "# Hide meanwhile for now\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.savefig(os.path.join(new_folder, 'plot2.png'))\n",
        "plt.show()\n",
        "\n",
        "# display the content of the model\n",
        "baseline_results = model.evaluate(x_test, y_test, verbose=2)\n",
        "for name, value in zip(model.metrics_names, baseline_results):\n",
        "    print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "# test a prediction\n",
        "test_predictions_baseline = model.predict(x_test)\n",
        "plotter.plot_confusion_matrix_generic(y_test, test_predictions_baseline, os.path.join(new_folder, 'plot3.png'), 0)\n",
        "\n",
        "# save the predictions\n",
        "prediction_writer = Prediction(test_predictions_baseline, 400, new_folder)\n",
        "prediction_writer.save()\n",
        "prediction_writer.save_all(y_test)\n",
        "\n",
        "# show the final score\n",
        "score = FinalScore(new_folder)\n",
        "score.output()\n",
        "\n",
        "# plot output results\n",
        "plotter.plot_output(test_predictions_baseline, y_test, x_test_drawing, os.path.join(new_folder, 'plot4.png'))"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZBmBRYuNjIo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "PKGBoVo6UtKB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import logging.config\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from absl import app\n",
        "from tensorflow.keras.applications import vgg16\n",
        "\n",
        "\n",
        "# def main(argv):\n",
        "def main():\n",
        "    print(tf.version.VERSION)\n",
        "    image_size = 224\n",
        "    test_run = 'zCSA2'\n",
        "    new_folder = r'/content/drive/MyDrive/AIMedic/OcularDisease/VGG16_basic_outputs/e1e12c704633d1af537b61ece82c2860/'\n",
        "\n",
        "    # load the data\n",
        "    (x_train, y_train), (x_test, y_test) = load_data(image_size, 1)\n",
        "\n",
        "    class_names = ['Normal', 'Diabetes', 'Glaucoma', 'Cataract', 'AMD', 'Hypertension', 'Myopia', 'Others']\n",
        "\n",
        "    # plot data input\n",
        "    plotter = Plotter(class_names)\n",
        "    plotter.plot_input_images(x_test, y_test)\n",
        "\n",
        "    x_test_drawing = x_test\n",
        "\n",
        "    # normalize input based on model\n",
        "    x_test = vgg16.preprocess_input(x_test)\n",
        "\n",
        "    # load one of the test runs\n",
        "    model = tf.keras.models.load_model(os.path.join(new_folder , 'model_weights.h5'))\n",
        "    model.summary()\n",
        "\n",
        "    # display the content of the model\n",
        "    baseline_results = model.evaluate(x_test, y_test, verbose=2)\n",
        "    for name, value in zip(model.metrics_names, baseline_results):\n",
        "        print(name, ': ', value)\n",
        "    print()\n",
        "\n",
        "    # test a prediction\n",
        "    test_predictions_baseline = model.predict(x_test)\n",
        "    plotter.plot_confusion_matrix_generic(y_test, test_predictions_baseline, test_run, 0)\n",
        "\n",
        "    # save the predictions\n",
        "    prediction_writer = Prediction(test_predictions_baseline, 400)\n",
        "    prediction_writer.save()\n",
        "    prediction_writer.save_all(y_test)\n",
        "\n",
        "    # show the final score\n",
        "    score = FinalScore(new_folder)\n",
        "    score.output()\n",
        "\n",
        "    # plot output results\n",
        "    plotter.plot_output(test_predictions_baseline, y_test, x_test_drawing, test_run)"
      ],
      "outputs": [],
      "metadata": {
        "id": "HMtgsxfuUunw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# create logger\n",
        "logging.config.fileConfig('logging.conf')\n",
        "logger = logging.getLogger('odir')\n",
        "# app.run(main)\n",
        "main()"
      ],
      "outputs": [],
      "metadata": {
        "id": "BoAhsXY2aj5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG19"
      ],
      "metadata": {
        "id": "uGXgYxBUmHvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Model"
      ],
      "metadata": {
        "id": "kBmN0B0Vrcdn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "# os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
        "import secrets\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications import vgg19\n",
        "\n",
        "# batch_size = 32\n",
        "batch_size = 16\n",
        "num_classes = 8\n",
        "# epochs = 50\n",
        "epochs = 1\n",
        "patience = 5\n",
        "\n",
        "class_weight = {0: 1.,\n",
        "                1: 1.583802025,\n",
        "                2: 8.996805112,\n",
        "                3: 10.24,\n",
        "                4: 10.05714286,\n",
        "                5: 1.,\n",
        "                6: 1.,\n",
        "                7: 2.505338078}\n",
        "\n",
        "token = secrets.token_hex(16)\n",
        "folder = r'/content/drive/MyDrive/AIMedic/OcularDisease/VGG19_basic_outputs'\n",
        "\n",
        "new_folder = os.path.join(folder, token)\n",
        "\n",
        "if not os.path.exists(new_folder):\n",
        "    os.makedirs(new_folder)\n",
        "\n",
        "defined_metrics = [\n",
        "    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "    tf.keras.metrics.Precision(name='precision'),\n",
        "    tf.keras.metrics.Recall(name='recall'),\n",
        "    tf.keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "\n",
        "factory = Factory((224, 224, 3), defined_metrics)\n",
        "model = factory.compile(ModelTypes.vgg19)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_data(224, 1)\n",
        "\n",
        "x_test_drawing = x_test\n",
        "\n",
        "#_______________________below commented____________________________\n",
        "# x_train = vgg19.preprocess_input(x_train)\n",
        "# x_test = vgg19.preprocess_input(x_test)\n",
        "\n",
        "class_names = ['Normal', 'Diabetes', 'Glaucoma', 'Cataract', 'AMD', 'Hypertension', 'Myopia', 'Others']\n",
        "\n",
        "# plot data input\n",
        "plotter = Plotter(class_names)\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min', verbose=1)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "8vCqLhdtnR35"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True, #class_weight=class_weight,\n",
        "                    validation_data=(x_test, y_test), callbacks=[callback])"
      ],
      "outputs": [],
      "metadata": {
        "id": "N55LOYowqQdT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"saving weights\")\n",
        "model.save(os.path.join(new_folder, 'model_weights.h5'))\n",
        "\n",
        "print(\"plotting metrics\")\n",
        "plotter.plot_metrics(history, os.path.join(new_folder, 'plot1.png'), 2)\n",
        "\n",
        "print(\"plotting accuracy\")\n",
        "plotter.plot_accuracy(history, os.path.join(new_folder, 'plot2.png'))\n",
        "\n",
        "print(\"display the content of the model\")\n",
        "baseline_results = model.evaluate(x_test, y_test, verbose=2)\n",
        "for name, value in zip(model.metrics_names, baseline_results):\n",
        "    print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "# test a prediction\n",
        "test_predictions_baseline = model.predict(x_test)\n",
        "print(\"plotting confusion matrix\")\n",
        "plotter.plot_confusion_matrix_generic(y_test, test_predictions_baseline, os.path.join(new_folder, 'plot3.png'), 0)\n",
        "\n",
        "# save the predictions\n",
        "prediction_writer = Prediction(test_predictions_baseline, 400, new_folder)\n",
        "prediction_writer.save()\n",
        "prediction_writer.save_all(y_test)\n",
        "\n",
        "# show the final score\n",
        "score = FinalScore(new_folder)\n",
        "score.output()\n",
        "\n",
        "# plot output results\n",
        "plotter.plot_output(test_predictions_baseline, y_test, x_test_drawing, os.path.join(new_folder, 'plot4.png'))"
      ],
      "outputs": [],
      "metadata": {
        "id": "1VP4WuOvqORI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet50"
      ],
      "metadata": {
        "id": "r9OAGC5amooW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Model"
      ],
      "metadata": {
        "id": "DulHadWqncAH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import resnet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
        "import secrets\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# batch_size = 32\n",
        "num_classes = 8\n",
        "# epochs = 100\n",
        "patience = 8\n",
        "\n",
        "class_weight = {0: 1.,\n",
        "                1: 1.583802025,\n",
        "                2: 8.996805112,\n",
        "                3: 10.24,\n",
        "                4: 10.05714286,\n",
        "                5: 1.,\n",
        "                6: 1.,\n",
        "                7: 2.505338078}\n",
        "\n",
        "token = secrets.token_hex(16)\n",
        "folder = r'/content/drive/MyDrive/AIMedic/OcularDisease/ResNet50_basic_outputs'\n",
        "\n",
        "new_folder = os.path.join(folder, token)\n",
        "\n",
        "if not os.path.exists(new_folder):\n",
        "    os.makedirs(new_folder)\n",
        "\n",
        "base_model = resnet50.ResNet50\n",
        "\n",
        "base_model = base_model(weights='imagenet', include_top=False)\n",
        "\n",
        "# Comment this out if you want to train all layers\n",
        "#for layer in base_model.layers:\n",
        "#    layer.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(num_classes, activation='sigmoid')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "model.summary()\n",
        "\n",
        "tf.keras.utils.plot_model(model, to_file=os.path.join(new_folder, 'model_resnet50.png'), show_shapes=True, show_layer_names=True)\n",
        "\n",
        "defined_metrics = [\n",
        "    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "    tf.keras.metrics.Precision(name='precision'),\n",
        "    tf.keras.metrics.Recall(name='recall'),\n",
        "    tf.keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "\n",
        "# Adam Optimizer Example\n",
        "# model.compile(loss='binary_crossentropy',\n",
        "#               optimizer=Adam(lr=0.001),\n",
        "#               metrics=defined_metrics)\n",
        "\n",
        "# RMSProp Optimizer Example\n",
        "# model.compile(loss='binary_crossentropy',\n",
        "#               optimizer='rmsprop',\n",
        "#               metrics=defined_metrics)\n",
        "\n",
        "sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "print('Configuration Start -------------------------')\n",
        "print(sgd.get_config())\n",
        "print('Configuration End -------------------------')\n",
        "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=defined_metrics)\n",
        "\n",
        "#________________exists above_____________\n",
        "# (x_train, y_train), (x_test, y_test) = load_data(224)\n",
        "\n",
        "x_test_drawing = x_test\n",
        "\n",
        "#_____________________below commented____________________\n",
        "# x_train = resnet50.preprocess_input(x_train)\n",
        "# x_test = resnet50.preprocess_input(x_test)\n",
        "\n",
        "class_names = ['Normal', 'Diabetes', 'Glaucoma', 'Cataract', 'AMD', 'Hypertension', 'Myopia', 'Others']\n",
        "\n",
        "# plot data input\n",
        "plotter = Plotter(class_names)\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min', verbose=1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "CR4-OKPVmp-x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True, #class_weight=class_weight,\n",
        "                    validation_data=(x_test, y_test), callbacks=[callback])"
      ],
      "outputs": [],
      "metadata": {
        "id": "vuDUCD8RsC-r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"saving weights\")\n",
        "model.save(os.path.join(new_folder, 'model_weights.h5'))\n",
        "\n",
        "print(\"plotting metrics\")\n",
        "plotter.plot_metrics(history, os.path.join(new_folder, 'plot1.png'), 2)\n",
        "\n",
        "print(\"plotting accuracy\")\n",
        "plotter.plot_accuracy(history, os.path.join(new_folder, 'plot2.png'))\n",
        "\n",
        "print(\"display the content of the model\")\n",
        "baseline_results = model.evaluate(x_test, y_test, verbose=2)\n",
        "for name, value in zip(model.metrics_names, baseline_results):\n",
        "    print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "# test a prediction\n",
        "test_predictions_baseline = model.predict(x_test)\n",
        "print(\"plotting confusion matrix\")\n",
        "plotter.plot_confusion_matrix_generic(y_test, test_predictions_baseline, os.path.join(new_folder, 'plot3.png'), 0)\n",
        "\n",
        "# save the predictions\n",
        "prediction_writer = Prediction(test_predictions_baseline, 400, new_folder)\n",
        "prediction_writer.save()\n",
        "prediction_writer.save_all(y_test)\n",
        "\n",
        "# show the final score\n",
        "score = FinalScore(new_folder)\n",
        "score.output()\n",
        "\n",
        "# plot output results\n",
        "plotter.plot_output(test_predictions_baseline, y_test, x_test_drawing, os.path.join(new_folder, 'plot4.png'))"
      ],
      "outputs": [],
      "metadata": {
        "id": "WA_h-xKZsAwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# InceptionResNetV2"
      ],
      "metadata": {
        "id": "u5Vvf701mqir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Model"
      ],
      "metadata": {
        "id": "hB9tbbwunfBI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import inception_resnet_v2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "\n",
        "# os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
        "import secrets\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "# batch_size = 32\n",
        "batch_size = 16\n",
        "num_classes = 8\n",
        "# epochs = 100\n",
        "epochs = 3\n",
        "patience = 3\n",
        "freeze_layers = 2\n",
        "\n",
        "# class_weight = {0: 1.,\n",
        "#                 1: 1.583802025,\n",
        "#                 2: 8.996805112,\n",
        "#                 3: 10.24,\n",
        "#                 4: 10.05714286,\n",
        "#                 5: 1.,\n",
        "#                 6: 1.,\n",
        "#                 7: 2.505338078}\n",
        "\n",
        "token = secrets.token_hex(16)\n",
        "folder = r'/content/drive/MyDrive/AIMedic/OcularDisease/InceptionResNetV2_basic_outputs'\n",
        "\n",
        "new_folder = os.path.join(folder, token)\n",
        "\n",
        "if not os.path.exists(new_folder):\n",
        "    os.makedirs(new_folder)\n",
        "\n",
        "base_model = inception_resnet_v2.InceptionResNetV2\n",
        "\n",
        "base_model = base_model(weights='imagenet', include_top=False)\n",
        "\n",
        "# Comment this out if you want to train all layers\n",
        "# for layer in base_model.layers:\n",
        "#     layer.trainable = True\n",
        "\n",
        "x = base_model.output\n",
        "#x = Flatten()(x)\n",
        "#x = Dropout(0.5)(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(num_classes, activation='sigmoid')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "#for layer in model.layers[:freeze_layers]:\n",
        "#    layer.trainable = True\n",
        "#for layer in model.layers[freeze_layers:]:\n",
        "#    layer.trainable = False\n",
        "\n",
        "model.summary()\n",
        "\n",
        "tf.keras.utils.plot_model(model, to_file=os.path.join(new_folder, 'model_inception_resnet_v2.png'), show_shapes=True, show_layer_names=True)\n",
        "\n",
        "defined_metrics = [\n",
        "    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "    tf.keras.metrics.Precision(name='precision'),\n",
        "    tf.keras.metrics.Recall(name='recall'),\n",
        "    tf.keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "\n",
        "# Adam Optimizer Example\n",
        "# model.compile(loss='binary_crossentropy',\n",
        "#               optimizer=Adam(lr=0.001),\n",
        "#               metrics=defined_metrics)\n",
        "\n",
        "# RMSProp Optimizer Example\n",
        "# model.compile(loss='binary_crossentropy',\n",
        "#               optimizer='rmsprop',\n",
        "#               metrics=defined_metrics)\n",
        "\n",
        "sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "print('Configuration Start -------------------------')\n",
        "print(sgd.get_config())\n",
        "print('Configuration End -------------------------')\n",
        "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=defined_metrics)\n",
        "\n",
        "#________________exists above____________________\n",
        "(x_train, y_train), (x_test, y_test) = load_data(224, 1)\n",
        "\n",
        "x_test_drawing = x_test\n",
        "\n",
        "#______________________below commented_____________________\n",
        "# x_train = inception_resnet_v2.preprocess_input(x_train)\n",
        "# x_test = inception_resnet_v2.preprocess_input(x_test)\n",
        "\n",
        "class_names = ['Normal', 'Diabetes', 'Glaucoma', 'Cataract', 'AMD', 'Hypertension', 'Myopia', 'Others']\n",
        "\n",
        "# plot data input\n",
        "plotter = Plotter(class_names)\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min', verbose=1)\n",
        "\n",
        "#class_weight = class_weight.compute_class_weight('balanced', np.unique(x_train), x_train)"
      ],
      "outputs": [],
      "metadata": {
        "id": "VKosBeLtmuXz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True, #class_weight= class_weight,\n",
        "                    validation_data=(x_test, y_test), callbacks=[callback])"
      ],
      "outputs": [],
      "metadata": {
        "id": "JZoNkPWIstoT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"saving weights\")\n",
        "model.save(os.path.join(new_folder, 'model_weights.h5'))\n",
        "\n",
        "print(\"plotting metrics\")\n",
        "plotter.plot_metrics(history, os.path.join(new_folder, 'plot1.png'), 2)\n",
        "\n",
        "print(\"plotting accuracy\")\n",
        "plotter.plot_accuracy(history, os.path.join(new_folder, 'plot2.png'))\n",
        "\n",
        "print(\"display the content of the model\")\n",
        "baseline_results = model.evaluate(x_test, y_test, verbose=2)\n",
        "for name, value in zip(model.metrics_names, baseline_results):\n",
        "    print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "# test a prediction\n",
        "test_predictions_baseline = model.predict(x_test)\n",
        "print(\"plotting confusion matrix\")\n",
        "plotter.plot_confusion_matrix_generic(y_test, test_predictions_baseline, os.path.join(new_folder, 'plot3.png'), 0)\n",
        "\n",
        "# save the predictions\n",
        "prediction_writer = Prediction(test_predictions_baseline, 400, new_folder)\n",
        "prediction_writer.save()\n",
        "prediction_writer.save_all(y_test)\n",
        "\n",
        "# show the final score\n",
        "score = FinalScore(new_folder)\n",
        "score.output()"
      ],
      "outputs": [],
      "metadata": {
        "id": "HNZrmTf0srwX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# plot output results\n",
        "plotter.plot_output(test_predictions_baseline, y_test, x_test_drawing, os.path.join(new_folder, 'plot4.png'))"
      ],
      "outputs": [],
      "metadata": {
        "id": "4mZthceFy7a-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "8rbyRcox3NF8"
      }
    }
  ]
}